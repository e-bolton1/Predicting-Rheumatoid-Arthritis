{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path  # To handle and manipulate filesystem paths\n",
    "import os  # For interacting with the operating system\n",
    "import glob  # For finding all file paths matching a specified pattern\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np  # For numerical operations and handling arrays\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\n",
    "from PIL import Image  # For opening, manipulating, and saving many different image file formats\n",
    "\n",
    "# PyTorch imports\n",
    "import torch  # Main PyTorch library for building and training neural networks\n",
    "from torch.utils.data import Dataset, DataLoader  # For handling datasets and data loaders\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch-I/O extension\n",
    "import torchio as tio  # For medical image processing in PyTorch\n",
    "\n",
    "# pydicom imports\n",
    "import pydicom  # For reading, modifying, and writing DICOM files\n",
    "from pydicom.data import get_testdata_file  # For accessing test DICOM files\n",
    "from pydicom.fileset import FileSet  # For working with DICOM FileSets\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split  # For splitting datasets into training and testing sets\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/CCP_MRI_image_subset\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/eleanorbolton/OneDrive - University of Leeds/CCP_MRI_IMAGE_SUBSET/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the DICOM Image\n",
    "This function processes a DICOM image and returns the image as a NumPy array. It optionally resizes the image to reduce its size in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dicom_image(path: str, resize=True) -> np.ndarray:\n",
    "    \"\"\" Given a path to a DICOM image, process and return the image. \n",
    "        Reduces the size in memory.\n",
    "    \"\"\"\n",
    "    dicom_file = pydicom.dcmread(path)\n",
    "    image = dicom_file.pixel_array\n",
    "    image = image - np.min(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    '''\n",
    "    # resize the image to 256x256 using PIL\n",
    "    if resize:\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((256, 256))\n",
    "        image = np.array(image)\n",
    "    '''\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Sequence Image\n",
    "This function returns a sorted list of images from a specified MRI sequence subfolder. It excludes images that are entirely black.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_images(path: str) -> list:\n",
    "    images = []\n",
    "    \n",
    "    # Get a list of all DICOM files in the directory\n",
    "    image_path_list = glob.glob(os.path.join(path, '*'))\n",
    "    \n",
    "    # Read the DICOM files and store them with their instance numbers\n",
    "    dicom_files = []\n",
    "    for image_path in image_path_list:\n",
    "        try:\n",
    "            dicom_file = pydicom.dcmread(image_path)\n",
    "            instance_number = dicom_file.InstanceNumber\n",
    "            dicom_files.append((instance_number, image_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {image_path}: {e}\")\n",
    "    \n",
    "    # Sort the files by instance number\n",
    "    dicom_files.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Read the pixel data in sorted order\n",
    "    for _, image_path in dicom_files:\n",
    "        try:\n",
    "            dicom_file = pydicom.dcmread(image_path)\n",
    "            image = dicom_file.pixel_array\n",
    "            images.append(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading pixel data from {image_path}: {e}\")\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the central slice\n",
    "The anatomical \"middle\" of the MR image will be different in each subject. we therefore need to decide the best way to define the central slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best slice\n",
    "\n",
    "This is based on the sum of the pixel tensor and finds the max sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_slice(dicom_files):\n",
    "    \"\"\" Find the slice with the highest sum of pixel intensities. \"\"\"\n",
    "    max_sum = -1\n",
    "    best_slice = None\n",
    "\n",
    "    for dicom_file, image_path in dicom_files:\n",
    "        try:\n",
    "            image = dicom_file.pixel_array\n",
    "            image_sum = np.sum(image)\n",
    "            if image_sum > max_sum:\n",
    "                max_sum = image_sum\n",
    "                best_slice = (dicom_file, image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "    return best_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicate Images\n",
    "Some images are present for the same subjects at the same position but have been processed. This Function removes the least infomrative of the duplicate image based on the number of 0 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dicom_files):\n",
    "    \"\"\" Remove duplicate instance numbers, keeping only the slice with the highest sum of intensities. \"\"\"\n",
    "    instance_dict = defaultdict(list)\n",
    "\n",
    "    for dicom_file, image_path in dicom_files:\n",
    "        instance_number = dicom_file.InstanceNumber\n",
    "        instance_dict[instance_number].append((dicom_file, image_path))\n",
    "\n",
    "    # Keep only the slice with the highest sum of intensities for each instance number\n",
    "    unique_dicom_files = []\n",
    "    for instance_number, files in instance_dict.items():\n",
    "        if len(files) > 1:\n",
    "            best_slice = find_best_slice(files)\n",
    "            unique_dicom_files.append(best_slice)\n",
    "        else:\n",
    "            unique_dicom_files.append(files[0])\n",
    "\n",
    "    return unique_dicom_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best subject Images:\n",
    "Selects the best images and surrounding images (based on seq_len) according to the sum of the intensities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_patient_images(base_path):\n",
    "    \"\"\" \n",
    "    Process all images in the 't1_vibe_we' subfolder of each subject.\n",
    "    Sort images by Instance Number and return a sequence of a fixed length.\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): Base path containing all subject folders.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of images for each subject that meet the criteria.\n",
    "    \"\"\"\n",
    "    seq_len = 20\n",
    "    all_images = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 't1_vibe_we' in dirs:\n",
    "            t1_vibe_we_path = os.path.join(root, 't1_vibe_we')\n",
    "            \n",
    "            # Get the images in the 't1_vibe_we' sequence\n",
    "            dicom_files = []\n",
    "            for image_path in glob.glob(os.path.join(t1_vibe_we_path, '*')):\n",
    "                try:\n",
    "                    dicom_file = pydicom.dcmread(image_path)\n",
    "                    dicom_files.append((dicom_file, image_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "            # Sort the files by Instance Number\n",
    "            dicom_files.sort(key=lambda x: x[0].InstanceNumber)\n",
    "            \n",
    "            # Remove duplicates\n",
    "            dicom_files = remove_duplicates(dicom_files)\n",
    "\n",
    "            # Find the best slice\n",
    "            best_slice = find_best_slice(dicom_files)\n",
    "            if best_slice:\n",
    "                best_dicom_file, best_image_path = best_slice\n",
    "                best_instance_number = best_dicom_file.InstanceNumber\n",
    "                print(f\"Best instance number: {best_instance_number}\")\n",
    "\n",
    "                # Calculate the start and end indices for the selected sequence\n",
    "                start_index = max(0, best_instance_number - (seq_len // 2))\n",
    "                end_index = start_index + seq_len\n",
    "\n",
    "                # Select the slices around the best slice\n",
    "                selected_slices = dicom_files[start_index:end_index]\n",
    "\n",
    "                images = []\n",
    "                for dicom_file, image_path in selected_slices:\n",
    "                    try:\n",
    "                        image = process_dicom_image(image_path)\n",
    "                        images.append(image)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "                # Determine the original image dimensions\n",
    "                if images:\n",
    "                    img_shape = images[0].shape\n",
    "\n",
    "                if len(images) < seq_len:\n",
    "                    # Pad with zero images of the same shape as the original images\n",
    "                    diff = seq_len - len(images)\n",
    "                    images.extend([np.zeros(img_shape, dtype=np.uint8) for _ in range(diff)])\n",
    "\n",
    "                all_images.extend(images)\n",
    "\n",
    "    return np.array(all_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV file and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file\n",
    "training_data_dir = \"/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/\" \n",
    "csv_path = os.path.join(training_data_dir, 'training_labels_subset.csv')\n",
    "labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, valid_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['progression'])\n",
    "\n",
    "# Save the splits for reference\n",
    "train_df.to_csv(os.path.join(training_data_dir, 'train_split.csv'), index=False)\n",
    "valid_df.to_csv(os.path.join(training_data_dir, 'valid_split.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandScanDataset(Dataset):\n",
    "    def __init__(self, labels_df, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels_df (DataFrame): DataFrame containing the patient IDs and labels\n",
    "            data_dir (str): Path to the data folder\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a list of patient IDs and their corresponding labels\n",
    "        self.patient_ids = self.labels_df['patient ID'].astype(str).str.zfill(5).tolist()\n",
    "        self.labels = self.labels_df['progression'].apply(lambda x: 1 if x == 'y' else 0).tolist()\n",
    "\n",
    "        # Create a dictionary of the labels\n",
    "        self.dict_labels = dict(zip(self.patient_ids, self.labels))\n",
    "        print(self.dict_labels)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Process the images for this patient\n",
    "        patient_dir = os.path.join(self.data_dir, patient_id)\n",
    "        images = get_best_patient_images(patient_dir)  # Ensure this function only returns images for the given patient\n",
    "        \n",
    "        # If no images were returned, handle this case (optional)\n",
    "        if len(images) == 0:\n",
    "            raise ValueError(f\"No images found for patient {patient_id}\")\n",
    "\n",
    "        print(f\"Patient ID: {patient_id}, Image shape: {images.shape}\")\n",
    "\n",
    "        if self.transform:\n",
    "            images = self.transform(images)\n",
    "\n",
    "        images_tensor = torch.tensor(images, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return images_tensor, label_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up transformation using torch.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom thresholding \n",
    "Separates the the foregrounds (objects of interest – hand) from the background \n",
    "Pixels with intensity values above this threshold are considered part of the foreground, while those below are treated as background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomThresholding(tio.Transform):\n",
    "    def __init__(self, threshold_percentage=0.1):\n",
    "        super().__init__()\n",
    "        self.threshold_percentage = threshold_percentage\n",
    "\n",
    "    def apply_transform(self, subject):\n",
    "        for key, image in subject.get_images_dict(intensity_only=True).items():\n",
    "            max_intensity = torch.max(image.data)\n",
    "            threshold_value = self.threshold_percentage * max_intensity\n",
    "            binary_mask = (image.data > threshold_value).float()\n",
    "            subject.add_image(tio.LabelMap(tensor=binary_mask), f'{key}_mask')\n",
    "        return subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morphological Operations\n",
    "improves the mask of the hand to identfiy the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphologicalOperations(tio.Transform):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel = torch.ones((1, 1, kernel_size, kernel_size, kernel_size), dtype=torch.float32)\n",
    "\n",
    "    def apply_transform(self, subject):\n",
    "        for key, image in subject.get_images_dict(intensity_only=False).items():\n",
    "            if 'mask' in key:\n",
    "                # Add batch and channel dimensions\n",
    "                mask_tensor = image.data.unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                # Morphological opening (erosion followed by dilation)\n",
    "                eroded = F.conv3d(mask_tensor, self.kernel, padding=1) > (self.kernel_size ** 3 - 1)\n",
    "                dilated = F.conv3d(eroded.float(), self.kernel, padding=1) > 0\n",
    "                \n",
    "                # Morphological closing (dilation followed by erosion)\n",
    "                dilated_closed = F.conv3d(dilated.float(), self.kernel, padding=1) > 0\n",
    "                eroded_closed = F.conv3d(dilated_closed.float(), self.kernel, padding=1) > (self.kernel_size ** 3 - 1)\n",
    "                \n",
    "                # Remove batch and channel dimensions\n",
    "                final_mask = eroded_closed.squeeze(0).squeeze(0)\n",
    "                subject.add_image(tio.LabelMap(tensor=final_mask), f'{key}_processed')\n",
    "        return subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the preprocessing and augmentation transforms\n",
    "transform = tio.Compose([\n",
    "    tio.ToCanonical(),                # Reorient images to a standard orientation\n",
    "    tio.CropOrPad((256, 256, 256)),   # Crop or pad images to the desired shape\n",
    "    CustomThresholding(threshold_percentage=0.1),  # Apply custom thresholding\n",
    "    MorphologicalOperations(kernel_size=3),        # Apply morphological operations\n",
    "    tio.RandomAffine(),               # Random affine transformations\n",
    "    tio.RandomElasticDeformation(),   # Random elastic deformations\n",
    "    tio.RandomFlip(axes=(0,)),        # Randomly flip along the first axis\n",
    "    tio.RandomNoise(),                # Add random Gaussian noise\n",
    "    tio.RandomBlur(),                 # Apply random blur\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CCP_120': 1, 'CCP_117': 0, 'CCP_73': 0, 'CCP_386': 1, 'CCP_644': 1, 'CCP_NG_166': 0, 'CCP_NG_42': 1, 'CCP_138': 0, 'CCP_874': 0, 'CCP_907': 1, 'CCP_NG_137': 1, 'CCP_647': 1, 'CCP_262': 0, 'CCP_541': 0, 'CCP_89': 0, 'CCP_202': 0, 'CCP_NG_56': 0, 'CCP_821': 1, 'CCP_66': 0, 'CCP_471': 0, 'CCP_34': 0, 'CCP_181': 1, 'CCP_557': 1, 'CCP_416': 0, 'CCP_NG_207': 0, 'CCP_568': 0, 'CCP_753': 0, 'CCP_NG_181': 0, 'CCP_81': 0, 'CCP_415': 1, 'CCP_NG_8': 1, 'CCP_283': 1, 'CCP_906': 0, 'CCP_968': 0, 'CCP_664': 0, 'CCP_736': 0, 'CCP_355': 0, 'CCP_NG_104': 1, 'CCP_247': 1, 'CCP_NG_188': 1, 'CCP_976': 0, 'CCP_NG_214': 0, 'CCP_824': 1, 'CCP_62': 0, 'CCP_NG_36': 0, 'CCP_802': 0, 'CCP_NG_172': 0, 'CCP_873': 0, 'CCP_207': 0, 'CCP_167': 1, 'CCP_944': 1, 'CCP_133': 1, 'CCP_NG_60': 0, 'CCP_1000': 0, 'CCP_252': 1, 'CCP_672': 0, 'CCP_531': 0, 'CCP_NG_175': 1, 'CCP_NG_107': 1, 'CCP_53': 0, 'CCP_NG_106': 1, 'CCP_105': 0, 'CCP_507': 1, 'CCP_405': 0, 'CCP_172': 1, 'CCP_901': 1, 'CCP_212': 1, 'CCP_485': 0, 'CCP_185': 0, 'CCP_422': 0, 'CCP_245': 1, 'CCP_NG_86': 0, 'CCP_263': 0, 'CCP_NG_150': 0, 'CCP_354': 1, 'CCP_307': 0, 'CCP_668': 1, 'CCP_174': 0, 'CCP_NG_16': 0, 'CCP_NG_49': 0, 'CCP_505': 0, 'CCP_849': 1, 'CCP_573': 0, 'CCP_643': 0, 'CCP_47': 0, 'CCP_619': 1, 'CCP_NG_52': 0, 'CCP_879': 1, 'CCP_102': 1, 'CCP_752': 0, 'CCP_NG_116': 0, 'CCP_NG_85': 1, 'CCP_457': 1, 'CCP_NG_79': 1, 'CCP_229': 0, 'CCP_199': 0, 'CCP_NG_147': 1, 'CCP_828': 0, 'CCP_131': 1, 'CCP_NG_144': 0, 'CCP_794': 1, 'CCP_NG_178': 0, 'CCP_783': 1, 'CCP_100': 0, 'CCP_330': 1, 'CCP_859': 0, 'CCP_657': 1, 'CCP_393': 0, 'CCP_NG_102': 1, 'CCP_635': 1, 'CCP_426': 0, 'CCP_NG_9': 0, 'CCP_520': 0, 'CCP_780': 1, 'CCP_894': 0, 'CCP_412': 0, 'CCP_50': 1, 'CCP_NG_96': 0, 'CCP_NG_160': 1, 'CCP_638': 0, 'CCP_124': 1, 'CCP_864': 0, 'CCP_103': 1, 'CCP_NG_97': 1, 'CCP_321': 0, 'CCP_228': 1, 'CCP_631': 1, 'CCP_666': 0, 'CCP_71': 0, 'CCP_319': 0, 'CCP_616': 1, 'CCP_52': 0, 'CCP_191': 1, 'CCP_NG_68': 0, 'CCP_NG_140': 1, 'CCP_44': 1, 'CCP_827': 0, 'CCP_830': 1, 'CCP_419': 0, 'CCP_389': 0, 'CCP_909': 1, 'CCP_414': 1, 'CCP_884': 0, 'CCP_NG_169': 0, 'CCP_78': 1, 'CCP_523': 0, 'CCP_NG_117': 1, 'CCP_NG_177': 1, 'CCP_266': 0, 'CCP_28': 1, 'CCP_NG_174': 1, 'CCP_516': 0, 'CCP_646': 0, 'CCP_NG_29': 0, 'CCP_NG_59': 1, 'CCP_169': 1, 'CCP_695': 1, 'CCP_290': 1, 'CCP_NG_17': 1, 'CCP_NG_185': 1, 'CCP_352': 0, 'CCP_87': 0, 'CCP_304': 0, 'CCP_57': 1, 'CCP_420': 1, 'CCP_107': 0, 'CCP_82': 1, 'CCP_1001': 1, 'CCP_65': 0, 'CCP_NG_54': 1, 'CCP_1008': 1, 'CCP_519': 1}\n",
      "{'CCP_947': 1, 'CCP_402': 1, 'CCP_1018': 0, 'CCP_NG_100': 0, 'CCP_180': 0, 'CCP_NG_1': 0, 'CCP_565': 1, 'CCP_612': 0, 'CCP_NG_23': 1, 'CCP_153': 0, 'CCP_804': 0, 'CCP_NG_67': 0, 'CCP_371': 1, 'CCP_NG_77': 0, 'CCP_511': 1, 'CCP_444': 1, 'CCP_70': 1, 'CCP_NG_229': 0, 'CCP_279': 1, 'CCP_239': 0, 'CCP_56': 1, 'CCP_NG_143': 0, 'CCP_829': 0, 'CCP_NG_197': 1, 'CCP_NG_113': 0, 'CCP_168': 0, 'CCP_43': 0, 'CCP_380': 1, 'CCP_NG_122': 0, 'CCP_369': 1, 'CCP_94': 0, 'CCP_46': 0, 'CCP_214': 1, 'CCP_954': 0, 'CCP_510': 0, 'CCP_113': 1, 'CCP_969': 1, 'CCP_45': 1, 'CCP_NG_39': 0, 'CCP_508': 0, 'CCP_823': 1, 'CCP_349': 0, 'CCP_598': 1, 'CCP_846': 1}\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets\n",
    "train_dataset = HandScanDataset(labels_df=train_df, data_dir=training_data_dir)\n",
    "valid_dataset = HandScanDataset(labels_df=valid_df, data_dir=training_data_dir)\n",
    "\n",
    "# Creating data loaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best instance number: 55\n",
      "Patient ID: CCP_120, Image shape: (20, 512, 384)\n",
      "Sample 0: Image shape: torch.Size([20, 512, 384]), Label: 1\n",
      "Best instance number: 77\n",
      "Patient ID: CCP_117, Image shape: (20, 512, 288)\n",
      "Sample 1: Image shape: torch.Size([20, 512, 288]), Label: 0\n",
      "Best instance number: 84\n",
      "Patient ID: CCP_73, Image shape: (20, 512, 288)\n",
      "Sample 2: Image shape: torch.Size([20, 512, 288]), Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Check a few samples directly from the dataset\n",
    "for i in range(len(train_dataset)):\n",
    "    images, labels = train_dataset[i]\n",
    "    print(f\"Sample {i}: Image shape: {images.shape}, Label: {labels}\")\n",
    "    if i == 2:  # Check only the first 3 samples\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
