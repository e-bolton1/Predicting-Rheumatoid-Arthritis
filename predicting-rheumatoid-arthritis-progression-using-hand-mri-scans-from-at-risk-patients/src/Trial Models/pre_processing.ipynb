{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path  # To handle and manipulate filesystem paths\n",
    "import os  # For interacting with the operating system\n",
    "import glob  # For finding all file paths matching a specified pattern\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np  # For numerical operations and handling arrays\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\n",
    "from PIL import Image  # For opening, manipulating, and saving many different image file formats\n",
    "\n",
    "# PyTorch imports\n",
    "import torch  # Main PyTorch library for building and training neural networks\n",
    "from torch.utils.data import Dataset, DataLoader  # For handling datasets and data loaders\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch-I/O extension\n",
    "import torchio as tio  # For medical image processing in PyTorch\n",
    "\n",
    "# pydicom imports\n",
    "import pydicom  # For reading, modifying, and writing DICOM files\n",
    "from pydicom.data import get_testdata_file  # For accessing test DICOM files\n",
    "from pydicom.fileset import FileSet  # For working with DICOM FileSets\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split  # For splitting datasets into training and testing sets\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/CCP_MRI_image_subset\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/eleanorbolton/OneDrive - University of Leeds/CCP_MRI_IMAGE_SUBSET/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the DICOM Image\n",
    "This function processes a DICOM image and returns the image as a NumPy array. It optionally resizes the image to reduce its size in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dicom_image(path: str, resize=True) -> np.ndarray:\n",
    "    \"\"\" Given a path to a DICOM image, process and return the image. \n",
    "        Reduces the size in memory.\n",
    "    \"\"\"\n",
    "    dicom_file = pydicom.dcmread(path)\n",
    "    image = dicom_file.pixel_array\n",
    "    image = image - np.min(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # resize the image to 256x256 using PIL\n",
    "    if resize:\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((512, 384))\n",
    "        image = np.array(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Sequence Image\n",
    "This function returns a sorted list of images from a specified MRI sequence subfolder. It excludes images that are entirely black.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_images(path: str) -> list:\n",
    "    images = []\n",
    "    \n",
    "    # Get a list of all DICOM files in the directory\n",
    "    image_path_list = glob.glob(os.path.join(path, '*'))\n",
    "    \n",
    "    # Read the DICOM files and store them with their instance numbers\n",
    "    dicom_files = []\n",
    "    for image_path in image_path_list:\n",
    "        try:\n",
    "            dicom_file = pydicom.dcmread(image_path)\n",
    "            instance_number = dicom_file.InstanceNumber\n",
    "            dicom_files.append((instance_number, image_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {image_path}: {e}\")\n",
    "    \n",
    "    # Sort the files by instance number\n",
    "    dicom_files.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Read the pixel data in sorted order\n",
    "    for _, image_path in dicom_files:\n",
    "        try:\n",
    "            dicom_file = pydicom.dcmread(image_path)\n",
    "            image = dicom_file.pixel_array\n",
    "            images.append(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading pixel data from {image_path}: {e}\")\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the central slice\n",
    "The anatomical \"middle\" of the MR image will be different in each subject. we therefore need to decide the best way to define the central slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best slice\n",
    "\n",
    "This is based on the sum of the pixel tensor and finds the max sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_slice(dicom_files):\n",
    "    \"\"\" Find the slice with the highest sum of pixel intensities. \"\"\"\n",
    "    max_sum = -1\n",
    "    best_slice = None\n",
    "\n",
    "    for dicom_file, image_path in dicom_files:\n",
    "        try:\n",
    "            image = dicom_file.pixel_array\n",
    "            image_sum = np.sum(image)\n",
    "            if image_sum > max_sum:\n",
    "                max_sum = image_sum\n",
    "                best_slice = (dicom_file, image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "    return best_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicate Images\n",
    "Some images are present for the same subjects at the same position but have been processed. This Function removes the least infomrative of the duplicate image based on the number of 0 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dicom_files):\n",
    "    \"\"\" Remove duplicate instance numbers, keeping only the slice with the highest sum of intensities. \"\"\"\n",
    "    instance_dict = defaultdict(list)\n",
    "\n",
    "    for dicom_file, image_path in dicom_files:\n",
    "        instance_number = dicom_file.InstanceNumber\n",
    "        instance_dict[instance_number].append((dicom_file, image_path))\n",
    "\n",
    "    # Keep only the slice with the highest sum of intensities for each instance number\n",
    "    unique_dicom_files = []\n",
    "    for instance_number, files in instance_dict.items():\n",
    "        if len(files) > 1:\n",
    "            best_slice = find_best_slice(files)\n",
    "            unique_dicom_files.append(best_slice)\n",
    "        else:\n",
    "            unique_dicom_files.append(files[0])\n",
    "\n",
    "    return unique_dicom_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best subject Images:\n",
    "Selects the best images and surrounding images (based on seq_len) according to the sum of the intensities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_patient_images(base_path):\n",
    "    \"\"\" \n",
    "    Process all images in the 't1_vibe_we' subfolder of each subject.\n",
    "    Sort images by Instance Number and return a sequence of a fixed length.\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): Base path containing all subject folders.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of images for each subject that meet the criteria.\n",
    "    \"\"\"\n",
    "    seq_len = 20\n",
    "    all_images = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 't1_vibe_we' in dirs:\n",
    "            t1_vibe_we_path = os.path.join(root, 't1_vibe_we')\n",
    "            \n",
    "            # Get the images in the 't1_vibe_we' sequence\n",
    "            dicom_files = []\n",
    "            for image_path in glob.glob(os.path.join(t1_vibe_we_path, '*')):\n",
    "                try:\n",
    "                    dicom_file = pydicom.dcmread(image_path)\n",
    "                    dicom_files.append((dicom_file, image_path))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "            # Sort the files by Instance Number\n",
    "            dicom_files.sort(key=lambda x: x[0].InstanceNumber)\n",
    "            \n",
    "            # Remove duplicates\n",
    "            dicom_files = remove_duplicates(dicom_files)\n",
    "\n",
    "            # Find the best slice\n",
    "            best_slice = find_best_slice(dicom_files)\n",
    "            if best_slice:\n",
    "                best_dicom_file, best_image_path = best_slice\n",
    "                best_instance_number = best_dicom_file.InstanceNumber\n",
    "                print(f\"Best instance number: {best_instance_number}\")\n",
    "\n",
    "                # Calculate the start and end indices for the selected sequence\n",
    "                start_index = max(0, best_instance_number - (seq_len // 2))\n",
    "                end_index = start_index + seq_len\n",
    "\n",
    "                # Select the slices around the best slice\n",
    "                selected_slices = dicom_files[start_index:end_index]\n",
    "\n",
    "                images = []\n",
    "                for dicom_file, image_path in selected_slices:\n",
    "                    try:\n",
    "                        image = process_dicom_image(image_path)\n",
    "                        # Add channel dimension (1, x, y, z)\n",
    "                        # image = np.expand_dims(image, axis=0)\n",
    "                        images.append(image)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "                # Determine the original image dimensions\n",
    "                if images:\n",
    "                    img_shape = images[0].shape\n",
    "\n",
    "                if len(images) < seq_len:\n",
    "                    # Pad with zero images of the same shape as the original images\n",
    "                    diff = seq_len - len(images)\n",
    "                    images.extend([np.zeros(img_shape, dtype=np.uint8) for _ in range(diff)])\n",
    "\n",
    "                all_images.extend(images)\n",
    "                print(np.array(all_images).shape)\n",
    "    return np.array(all_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV file and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file\n",
    "training_data_dir = \"/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/\" \n",
    "csv_path = os.path.join(training_data_dir, 'training_labels_subset.csv')\n",
    "labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, valid_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['progression'])\n",
    "\n",
    "# Save the splits for reference\n",
    "train_df.to_csv(os.path.join(training_data_dir, 'train_split.csv'), index=False)\n",
    "valid_df.to_csv(os.path.join(training_data_dir, 'valid_split.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandScanDataset(Dataset):\n",
    "    def __init__(self, labels_df, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels_df (DataFrame): DataFrame containing the patient IDs and labels\n",
    "            data_dir (str): Path to the data folder\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a list of patient IDs and their corresponding labels\n",
    "        self.patient_ids = self.labels_df['patient ID'].astype(str).str.zfill(5).tolist()\n",
    "        self.labels = self.labels_df['progression'].apply(lambda x: 1 if x == 'y' else 0).tolist()\n",
    "\n",
    "        # Create a dictionary of the labels\n",
    "        self.dict_labels = dict(zip(self.patient_ids, self.labels))\n",
    "        print(self.dict_labels)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Process the images for this patient\n",
    "        patient_dir = os.path.join(self.data_dir, patient_id)\n",
    "        images = get_best_patient_images(patient_dir)  # Ensure this function only returns images for the given patient\n",
    "        \n",
    "        # If no images were returned, handle this case (optional)\n",
    "        if len(images) == 0:\n",
    "            raise ValueError(f\"No images found for patient {patient_id}\")\n",
    "\n",
    "        print(f\"Patient ID: {patient_id}, Image shape: {images.shape}\")\n",
    "\n",
    "\n",
    "        images_tensor = torch.tensor(images, dtype=torch.float32)\n",
    "        images_tensor_channel = torch.unsqueeze(images_tensor, 0)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            images_tensor_channel = self.transform(images_tensor_channel)\n",
    "\n",
    "        return images_tensor_channel, label_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up transformation using torch.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom thresholding \n",
    "Separates the the foregrounds (objects of interest â€“ hand) from the background \n",
    "Pixels with intensity values above this threshold are considered part of the foreground, while those below are treated as background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomThresholding(tio.Transform):\n",
    "    def __init__(self, threshold_percentage=0.1):\n",
    "        super().__init__()\n",
    "        self.threshold_percentage = threshold_percentage\n",
    "\n",
    "    def apply_transform(self, subject):\n",
    "        for key, image in subject.get_images_dict(intensity_only=True).items():\n",
    "            max_intensity = torch.max(image.data)\n",
    "            threshold_value = self.threshold_percentage * max_intensity\n",
    "            binary_mask = (image.data > threshold_value).float()\n",
    "            subject.add_image(tio.LabelMap(tensor=binary_mask), f'{key}_mask')\n",
    "        return subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morphological Operations\n",
    "improves the mask of the hand to identfiy the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphologicalOperations(tio.Transform):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel = torch.ones((1, 1, kernel_size, kernel_size, kernel_size), dtype=torch.float32)\n",
    "\n",
    "    def apply_transform(self, subject):\n",
    "        for key, image in subject.get_images_dict(intensity_only=False).items():\n",
    "            if 'mask' in key:\n",
    "                # Add batch and channel dimensions\n",
    "                mask_tensor = image.data.unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                # Morphological opening (erosion followed by dilation)\n",
    "                eroded = F.conv3d(mask_tensor, self.kernel, padding=1) > (self.kernel_size ** 3 - 1)\n",
    "                dilated = F.conv3d(eroded.float(), self.kernel, padding=1) > 0\n",
    "                \n",
    "                # Morphological closing (dilation followed by erosion)\n",
    "                dilated_closed = F.conv3d(dilated.float(), self.kernel, padding=1) > 0\n",
    "                eroded_closed = F.conv3d(dilated_closed.float(), self.kernel, padding=1) > (self.kernel_size ** 3 - 1)\n",
    "                \n",
    "                # Remove batch and channel dimensions\n",
    "                final_mask = eroded_closed.squeeze(0).squeeze(0)\n",
    "                subject.add_image(tio.LabelMap(tensor=final_mask), f'{key}_processed')\n",
    "        return subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tio.Compose([\n",
    "    tio.ToCanonical(),                # Reorient images to a standard orientation\n",
    "    tio.CropOrPad((96, 96, 96)),    # Crop or pad to 20 slices and 256x256 pixels\n",
    "    CustomThresholding(threshold_percentage=0.1),  # Apply custom thresholding\n",
    "    MorphologicalOperations(kernel_size=3),        # Apply morphological operations\n",
    "    tio.RandomAffine(),               # Random affine transformations\n",
    "    tio.RandomElasticDeformation(),   # Random elastic deformations\n",
    "    tio.RandomFlip(axes=(0,)),        # Randomly flip along the depth axis\n",
    "    tio.RandomNoise(),                # Add random Gaussian noise \n",
    "    tio.RandomBlur(),                 # Apply random blur\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_transform = tio.Compose([\n",
    "    tio.ToCanonical(),                # Reorient images to a standard orientation\n",
    "    tio.CropOrPad((96, 96, 96))   # Crop or pad images to the desired shape\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HandScanDataset2(Dataset):\n",
    "    def __init__(self, labels_df, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels_df (DataFrame): DataFrame containing the patient IDs and labels\n",
    "            data_dir (str): Path to the data folder\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a list of patient IDs and their corresponding labels\n",
    "        self.patient_ids = self.labels_df['patient ID'].astype(str).str.zfill(5).tolist()\n",
    "        self.labels = self.labels_df['progression'].apply(lambda x: 1 if x == 'y' else 0).tolist()\n",
    "\n",
    "        # Create a dictionary of the labels\n",
    "        self.dict_labels = dict(zip(self.patient_ids, self.labels))\n",
    "        print(self.dict_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Process the images for this patient\n",
    "        patient_dir = os.path.join(self.data_dir, patient_id)\n",
    "        images = self.get_best_patient_images(patient_dir)  # Call the internal method\n",
    "        \n",
    "        # If no images were returned, handle this case (optional)\n",
    "        if len(images) == 0:\n",
    "            raise ValueError(f\"No images found for patient {patient_id}\")\n",
    "\n",
    "        print(f\"Patient ID: {patient_id}, Image shape: {images.shape}\")\n",
    "\n",
    "        images_tensor = torch.tensor(images, dtype=torch.float32)\n",
    "        images_tensor_channel = torch.unsqueeze(images_tensor, 0)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            images_tensor_channel = self.transform(images_tensor_channel)\n",
    "\n",
    "        return images_tensor_channel, label_tensor\n",
    "\n",
    "    def get_best_patient_images(self, base_path):\n",
    "        \"\"\" \n",
    "        Process all images in the 't1_vibe_we' subfolder of each subject.\n",
    "        Sort images by Instance Number and return a sequence of a fixed length.\n",
    "        \"\"\"\n",
    "        seq_len = 96\n",
    "        all_images = []\n",
    "\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            if 't1_vibe_we' in dirs:\n",
    "                t1_vibe_we_path = os.path.join(root, 't1_vibe_we')\n",
    "                \n",
    "                # Get the images in the 't1_vibe_we' sequence\n",
    "                dicom_files = []\n",
    "                for image_path in glob.glob(os.path.join(t1_vibe_we_path, '*')):\n",
    "                    try:\n",
    "                        dicom_file = pydicom.dcmread(image_path)\n",
    "                        dicom_files.append((dicom_file, image_path))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "                # Sort the files by Instance Number\n",
    "                dicom_files.sort(key=lambda x: x[0].InstanceNumber)\n",
    "                \n",
    "                # Remove duplicates\n",
    "                dicom_files = self.remove_duplicates(dicom_files)\n",
    "\n",
    "                # Find the best slice\n",
    "                best_slice = self.find_best_slice(dicom_files)\n",
    "                if best_slice:\n",
    "                    best_dicom_file, best_image_path = best_slice\n",
    "                    best_instance_number = best_dicom_file.InstanceNumber\n",
    "                    print(f\"Best instance number: {best_instance_number}\")\n",
    "\n",
    "                    # Calculate the start and end indices for the selected sequence\n",
    "                    start_index = max(0, best_instance_number - (seq_len // 2))\n",
    "                    end_index = start_index + seq_len\n",
    "\n",
    "                    # Select the slices around the best slice\n",
    "                    selected_slices = dicom_files[start_index:end_index]\n",
    "\n",
    "                    images = []\n",
    "                    for dicom_file, image_path in selected_slices:\n",
    "                        try:\n",
    "                            image = self.process_dicom_image(image_path)\n",
    "                            images.append(image)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "                    # Determine the original image dimensions\n",
    "                    if images:\n",
    "                        img_shape = images[0].shape\n",
    "\n",
    "                    if len(images) < seq_len:\n",
    "                        # Pad with zero images of the same shape as the original images\n",
    "                        diff = seq_len - len(images)\n",
    "                        images.extend([np.zeros(img_shape, dtype=np.uint8) for _ in range(diff)])\n",
    "\n",
    "                    all_images.extend(images)\n",
    "        return np.array(all_images)\n",
    "\n",
    "    def remove_duplicates(self, dicom_files):\n",
    "        \"\"\" Remove duplicate instance numbers, keeping only the slice with the highest sum of intensities. \"\"\"\n",
    "        instance_dict = defaultdict(list)\n",
    "\n",
    "        for dicom_file, image_path in dicom_files:\n",
    "            instance_number = dicom_file.InstanceNumber\n",
    "            instance_dict[instance_number].append((dicom_file, image_path))\n",
    "\n",
    "        # Keep only the slice with the highest sum of intensities for each instance number\n",
    "        unique_dicom_files = []\n",
    "        for instance_number, files in instance_dict.items():\n",
    "            if len(files) > 1:\n",
    "                best_slice = self.find_best_slice(files)\n",
    "                unique_dicom_files.append(best_slice)\n",
    "            else:\n",
    "                unique_dicom_files.append(files[0])\n",
    "\n",
    "        return unique_dicom_files\n",
    "\n",
    "    def find_best_slice(self, dicom_files):\n",
    "        \"\"\" Find the slice with the highest sum of pixel intensities. \"\"\"\n",
    "        max_sum = -1\n",
    "        best_slice = None\n",
    "\n",
    "        for dicom_file, image_path in dicom_files:\n",
    "            try:\n",
    "                image = dicom_file.pixel_array\n",
    "                image_sum = np.sum(image)\n",
    "                if image_sum > max_sum:\n",
    "                    max_sum = image_sum\n",
    "                    best_slice = (dicom_file, image_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "        return best_slice\n",
    "\n",
    "    def process_dicom_image(self, path: str, resize=True) -> np.ndarray:\n",
    "        \"\"\" Given a path to a DICOM image, process and return the image. \n",
    "            Reduces the size in memory.\n",
    "        \"\"\"\n",
    "        dicom_file = pydicom.dcmread(path)\n",
    "        image = dicom_file.pixel_array\n",
    "        image = image - np.min(image)\n",
    "        image = image.astype(np.uint8)\n",
    "        \n",
    "        # Resize the image to 512x384 using PIL\n",
    "        if resize:\n",
    "            image = Image.fromarray(image)\n",
    "            image = image.resize((512, 384))\n",
    "            image = np.array(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def get_sequence_images(self, path: str) -> list:\n",
    "        images = []\n",
    "        \n",
    "        # Get a list of all DICOM files in the directory\n",
    "        image_path_list = glob.glob(os.path.join(path, '*'))\n",
    "        \n",
    "        # Read the DICOM files and store them with their instance numbers\n",
    "        dicom_files = []\n",
    "        for image_path in image_path_list:\n",
    "            try:\n",
    "                dicom_file = pydicom.dcmread(image_path)\n",
    "                instance_number = dicom_file.InstanceNumber\n",
    "                dicom_files.append((instance_number, image_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {image_path}: {e}\")\n",
    "        \n",
    "        # Sort the files by instance number\n",
    "        dicom_files.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Read the pixel data in sorted order\n",
    "        for _, image_path in dicom_files:\n",
    "            try:\n",
    "                dicom_file = pydicom.dcmread(image_path)\n",
    "                image = dicom_file.pixel_array\n",
    "                images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading pixel data from {image_path}: {e}\")\n",
    "        \n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CCP_120': 1, 'CCP_117': 0, 'CCP_73': 0, 'CCP_386': 1, 'CCP_644': 1, 'CCP_NG_166': 0, 'CCP_NG_42': 1, 'CCP_138': 0, 'CCP_874': 0, 'CCP_907': 1, 'CCP_NG_137': 1, 'CCP_647': 1, 'CCP_262': 0, 'CCP_541': 0, 'CCP_89': 0, 'CCP_202': 0, 'CCP_NG_56': 0, 'CCP_821': 1, 'CCP_66': 0, 'CCP_471': 0, 'CCP_34': 0, 'CCP_181': 1, 'CCP_557': 1, 'CCP_416': 0, 'CCP_NG_207': 0, 'CCP_568': 0, 'CCP_753': 0, 'CCP_NG_181': 0, 'CCP_81': 0, 'CCP_415': 1, 'CCP_NG_8': 1, 'CCP_283': 1, 'CCP_906': 0, 'CCP_968': 0, 'CCP_664': 0, 'CCP_736': 0, 'CCP_355': 0, 'CCP_NG_104': 1, 'CCP_247': 1, 'CCP_NG_188': 1, 'CCP_976': 0, 'CCP_NG_214': 0, 'CCP_824': 1, 'CCP_62': 0, 'CCP_NG_36': 0, 'CCP_802': 0, 'CCP_NG_172': 0, 'CCP_873': 0, 'CCP_207': 0, 'CCP_167': 1, 'CCP_944': 1, 'CCP_133': 1, 'CCP_NG_60': 0, 'CCP_1000': 0, 'CCP_252': 1, 'CCP_672': 0, 'CCP_531': 0, 'CCP_NG_175': 1, 'CCP_NG_107': 1, 'CCP_53': 0, 'CCP_NG_106': 1, 'CCP_105': 0, 'CCP_507': 1, 'CCP_405': 0, 'CCP_172': 1, 'CCP_901': 1, 'CCP_212': 1, 'CCP_485': 0, 'CCP_185': 0, 'CCP_422': 0, 'CCP_245': 1, 'CCP_NG_86': 0, 'CCP_263': 0, 'CCP_NG_150': 0, 'CCP_354': 1, 'CCP_307': 0, 'CCP_668': 1, 'CCP_174': 0, 'CCP_NG_16': 0, 'CCP_NG_49': 0, 'CCP_505': 0, 'CCP_849': 1, 'CCP_573': 0, 'CCP_643': 0, 'CCP_47': 0, 'CCP_619': 1, 'CCP_NG_52': 0, 'CCP_879': 1, 'CCP_102': 1, 'CCP_752': 0, 'CCP_NG_116': 0, 'CCP_NG_85': 1, 'CCP_457': 1, 'CCP_NG_79': 1, 'CCP_229': 0, 'CCP_199': 0, 'CCP_NG_147': 1, 'CCP_828': 0, 'CCP_131': 1, 'CCP_NG_144': 0, 'CCP_794': 1, 'CCP_NG_178': 0, 'CCP_783': 1, 'CCP_100': 0, 'CCP_330': 1, 'CCP_859': 0, 'CCP_657': 1, 'CCP_393': 0, 'CCP_NG_102': 1, 'CCP_635': 1, 'CCP_426': 0, 'CCP_NG_9': 0, 'CCP_520': 0, 'CCP_780': 1, 'CCP_894': 0, 'CCP_412': 0, 'CCP_50': 1, 'CCP_NG_96': 0, 'CCP_NG_160': 1, 'CCP_638': 0, 'CCP_124': 1, 'CCP_864': 0, 'CCP_103': 1, 'CCP_NG_97': 1, 'CCP_321': 0, 'CCP_228': 1, 'CCP_631': 1, 'CCP_666': 0, 'CCP_71': 0, 'CCP_319': 0, 'CCP_616': 1, 'CCP_52': 0, 'CCP_191': 1, 'CCP_NG_68': 0, 'CCP_NG_140': 1, 'CCP_44': 1, 'CCP_827': 0, 'CCP_830': 1, 'CCP_419': 0, 'CCP_389': 0, 'CCP_909': 1, 'CCP_414': 1, 'CCP_884': 0, 'CCP_NG_169': 0, 'CCP_78': 1, 'CCP_523': 0, 'CCP_NG_117': 1, 'CCP_NG_177': 1, 'CCP_266': 0, 'CCP_28': 1, 'CCP_NG_174': 1, 'CCP_516': 0, 'CCP_646': 0, 'CCP_NG_29': 0, 'CCP_NG_59': 1, 'CCP_169': 1, 'CCP_695': 1, 'CCP_290': 1, 'CCP_NG_17': 1, 'CCP_NG_185': 1, 'CCP_352': 0, 'CCP_87': 0, 'CCP_304': 0, 'CCP_57': 1, 'CCP_420': 1, 'CCP_107': 0, 'CCP_82': 1, 'CCP_1001': 1, 'CCP_65': 0, 'CCP_NG_54': 1, 'CCP_1008': 1, 'CCP_519': 1}\n",
      "{'CCP_947': 1, 'CCP_402': 1, 'CCP_1018': 0, 'CCP_NG_100': 0, 'CCP_180': 0, 'CCP_NG_1': 0, 'CCP_565': 1, 'CCP_612': 0, 'CCP_NG_23': 1, 'CCP_153': 0, 'CCP_804': 0, 'CCP_NG_67': 0, 'CCP_371': 1, 'CCP_NG_77': 0, 'CCP_511': 1, 'CCP_444': 1, 'CCP_70': 1, 'CCP_NG_229': 0, 'CCP_279': 1, 'CCP_239': 0, 'CCP_56': 1, 'CCP_NG_143': 0, 'CCP_829': 0, 'CCP_NG_197': 1, 'CCP_NG_113': 0, 'CCP_168': 0, 'CCP_43': 0, 'CCP_380': 1, 'CCP_NG_122': 0, 'CCP_369': 1, 'CCP_94': 0, 'CCP_46': 0, 'CCP_214': 1, 'CCP_954': 0, 'CCP_510': 0, 'CCP_113': 1, 'CCP_969': 1, 'CCP_45': 1, 'CCP_NG_39': 0, 'CCP_508': 0, 'CCP_823': 1, 'CCP_349': 0, 'CCP_598': 1, 'CCP_846': 1}\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets\n",
    "train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "valid_dataset = HandScanDataset2(labels_df=valid_df, data_dir=training_data_dir, transform=validation_transform)\n",
    "\n",
    "# Creating data loaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best instance number: 55\n",
      "Patient ID: CCP_120, Image shape: (96, 384, 512)\n",
      "Sample 0: Image shape: torch.Size([1, 96, 96, 96]), Label: 1\n",
      "Best instance number: 77\n",
      "Patient ID: CCP_117, Image shape: (96, 384, 512)\n",
      "Sample 1: Image shape: torch.Size([1, 96, 96, 96]), Label: 0\n",
      "Best instance number: 84\n",
      "Patient ID: CCP_73, Image shape: (96, 384, 512)\n",
      "Sample 2: Image shape: torch.Size([1, 96, 96, 96]), Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Check a few samples directly from the dataset\n",
    "for i in range(len(train_dataset)):\n",
    "    images, labels = train_dataset[i]\n",
    "    print(f\"Sample {i}: Image shape: {images.shape}, Label: {labels}\")\n",
    "    if i == 2:  # Check only the first 3 samples\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best instance number: 82\n",
      "Patient ID: CCP_657, Image shape: (96, 384, 512)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Iterate through the train_loader and print the shape of the batches\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch image shape: \u001b[39m\u001b[39m{\u001b[39;00mimages\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch label shape: \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Process the images for this patient\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m patient_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, patient_id)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_best_patient_images(patient_dir)  \u001b[39m# Call the internal method\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# If no images were returned, handle this case (optional)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(images) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb Cell 29\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_path \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(t1_vibe_we_path, \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         dicom_file \u001b[39m=\u001b[39m pydicom\u001b[39m.\u001b[39mdcmread(image_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         dicom_files\u001b[39m.\u001b[39mappend((dicom_file, image_path))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/data/pre_processing.ipynb#X40sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydicom/filereader.py:1030\u001b[0m, in \u001b[0;36mdcmread\u001b[0;34m(fp, defer_size, stop_before_pixels, force, specific_tags)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     stop_when \u001b[39m=\u001b[39m _at_pixel_data\n\u001b[1;32m   1029\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     dataset \u001b[39m=\u001b[39m read_partial(\n\u001b[1;32m   1031\u001b[0m         fp,\n\u001b[1;32m   1032\u001b[0m         stop_when,\n\u001b[1;32m   1033\u001b[0m         defer_size\u001b[39m=\u001b[39msize_in_bytes(defer_size),\n\u001b[1;32m   1034\u001b[0m         force\u001b[39m=\u001b[39mforce,\n\u001b[1;32m   1035\u001b[0m         specific_tags\u001b[39m=\u001b[39mspecific_tags,\n\u001b[1;32m   1036\u001b[0m     )\n\u001b[1;32m   1037\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m caller_owns_file:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydicom/filereader.py:778\u001b[0m, in \u001b[0;36mread_partial\u001b[0;34m(fileobj, stop_when, defer_size, force, specific_tags)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse a DICOM file until a condition is met.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \n\u001b[1;32m    747\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[39m    More generic file reading function.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m# Read File Meta Information\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[39m# Read preamble (if present)\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m preamble \u001b[39m=\u001b[39m read_preamble(fileobj, force)\n\u001b[1;32m    779\u001b[0m \u001b[39m# Read any File Meta Information group (0002,eeee) elements (if present)\u001b[39;00m\n\u001b[1;32m    780\u001b[0m file_meta \u001b[39m=\u001b[39m _read_file_meta_info(fileobj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydicom/filereader.py:705\u001b[0m, in \u001b[0;36mread_preamble\u001b[0;34m(fp, force)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the 128-byte DICOM preamble in `fp` if present.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[39m`fp` should be positioned at the start of the file-like. If the preamble\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mno header found.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mReading File Meta Information preamble...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m preamble \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread(\u001b[39m128\u001b[39m)\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mdebugging:\n\u001b[1;32m    707\u001b[0m     sample \u001b[39m=\u001b[39m bytes2hex(preamble[:\u001b[39m8\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m bytes2hex(preamble[\u001b[39m-\u001b[39m\u001b[39m8\u001b[39m:])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through the train_loader and print the shape of the batches\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch image shape: {images.shape}\")\n",
    "    print(f\"Batch label shape: {labels.shape}\")\n",
    "    break  # Remove this break to see all batches, or keep to see just the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
