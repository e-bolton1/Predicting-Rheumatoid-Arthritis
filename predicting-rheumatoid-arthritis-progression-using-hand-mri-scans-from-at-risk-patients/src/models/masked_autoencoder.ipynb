{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre_processing \n",
    "from pre_processing import HandScanDataset2, transform, validation_transform, train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvBlock3D, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Encoder3D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            ConvBlock3D(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            ConvBlock3D(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            ConvBlock3D(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Decoder3D, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, 128, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(MaskedAutoencoder3D, self).__init__()\n",
    "        self.encoder = Encoder3D(in_channels)\n",
    "        self.decoder = Decoder3D(256)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_masked = x * mask\n",
    "        encoded = self.encoder(x_masked)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(shape, mask_ratio=0.7):\n",
    "    mask = np.ones(shape)\n",
    "    num_elements = shape[1] * shape[2] * shape[3]\n",
    "    num_masked = int(mask_ratio * num_elements)\n",
    "    indices = np.random.choice(num_elements, num_masked, replace=False)\n",
    "    mask.reshape(-1)[indices] = 0\n",
    "    return torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.activation_count = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.activation_count += 1\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mask = create_mask(images.shape)\n",
    "        outputs = model(images, mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc = correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def val_step(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            mask = create_mask(images.shape)\n",
    "            outputs = model(images, mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(dataloader)\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          val_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5,\n",
    "          results_path: str = \"./results\"):\n",
    "\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=0.03)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer)\n",
    "        val_loss, val_acc = val_step(model, val_dataloader, loss_fn)\n",
    "\n",
    "        if early_stopper(val_loss):\n",
    "            if early_stopper.activation_count == 1:\n",
    "                print(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {results_path}/{timestamp}_optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\n",
    "                opt_MLP_filepath = f\"{results_path}/{timestamp}_optimal_model_weights_epoch_{epoch + 1}.pth\"\n",
    "                Early_Stop_Epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), opt_MLP_filepath)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"val_loss: {val_loss:.4f} | \"\n",
    "            f\"val_acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    return results, Early_Stop_Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CCP_120': 1, 'CCP_117': 0, 'CCP_73': 0, 'CCP_386': 1, 'CCP_644': 1, 'CCP_NG_166': 0, 'CCP_NG_42': 1, 'CCP_138': 0, 'CCP_874': 0, 'CCP_907': 1, 'CCP_NG_137': 1, 'CCP_647': 1, 'CCP_262': 0, 'CCP_541': 0, 'CCP_89': 0, 'CCP_202': 0, 'CCP_NG_56': 0, 'CCP_821': 1, 'CCP_66': 0, 'CCP_471': 0, 'CCP_34': 0, 'CCP_181': 1, 'CCP_557': 1, 'CCP_416': 0, 'CCP_NG_207': 0, 'CCP_568': 0, 'CCP_753': 0, 'CCP_NG_181': 0, 'CCP_81': 0, 'CCP_415': 1, 'CCP_NG_8': 1, 'CCP_283': 1, 'CCP_906': 0, 'CCP_968': 0, 'CCP_664': 0, 'CCP_736': 0, 'CCP_355': 0, 'CCP_NG_104': 1, 'CCP_247': 1, 'CCP_NG_188': 1, 'CCP_976': 0, 'CCP_NG_214': 0, 'CCP_824': 1, 'CCP_62': 0, 'CCP_NG_36': 0, 'CCP_802': 0, 'CCP_NG_172': 0, 'CCP_873': 0, 'CCP_207': 0, 'CCP_167': 1, 'CCP_944': 1, 'CCP_133': 1, 'CCP_NG_60': 0, 'CCP_1000': 0, 'CCP_252': 1, 'CCP_672': 0, 'CCP_531': 0, 'CCP_NG_175': 1, 'CCP_NG_107': 1, 'CCP_53': 0, 'CCP_NG_106': 1, 'CCP_105': 0, 'CCP_507': 1, 'CCP_405': 0, 'CCP_172': 1, 'CCP_901': 1, 'CCP_212': 1, 'CCP_485': 0, 'CCP_185': 0, 'CCP_422': 0, 'CCP_245': 1, 'CCP_NG_86': 0, 'CCP_263': 0, 'CCP_NG_150': 0, 'CCP_354': 1, 'CCP_307': 0, 'CCP_668': 1, 'CCP_174': 0, 'CCP_NG_16': 0, 'CCP_NG_49': 0, 'CCP_505': 0, 'CCP_849': 1, 'CCP_573': 0, 'CCP_643': 0, 'CCP_47': 0, 'CCP_619': 1, 'CCP_NG_52': 0, 'CCP_879': 1, 'CCP_102': 1, 'CCP_752': 0, 'CCP_NG_116': 0, 'CCP_NG_85': 1, 'CCP_457': 1, 'CCP_NG_79': 1, 'CCP_229': 0, 'CCP_199': 0, 'CCP_NG_147': 1, 'CCP_828': 0, 'CCP_131': 1, 'CCP_NG_144': 0, 'CCP_794': 1, 'CCP_NG_178': 0, 'CCP_783': 1, 'CCP_100': 0, 'CCP_330': 1, 'CCP_859': 0, 'CCP_657': 1, 'CCP_393': 0, 'CCP_NG_102': 1, 'CCP_635': 1, 'CCP_426': 0, 'CCP_NG_9': 0, 'CCP_520': 0, 'CCP_780': 1, 'CCP_894': 0, 'CCP_412': 0, 'CCP_50': 1, 'CCP_NG_96': 0, 'CCP_NG_160': 1, 'CCP_638': 0, 'CCP_124': 1, 'CCP_864': 0, 'CCP_103': 1, 'CCP_NG_97': 1, 'CCP_321': 0, 'CCP_228': 1, 'CCP_631': 1, 'CCP_666': 0, 'CCP_71': 0, 'CCP_319': 0, 'CCP_616': 1, 'CCP_52': 0, 'CCP_191': 1, 'CCP_NG_68': 0, 'CCP_NG_140': 1, 'CCP_44': 1, 'CCP_827': 0, 'CCP_830': 1, 'CCP_419': 0, 'CCP_389': 0, 'CCP_909': 1, 'CCP_414': 1, 'CCP_884': 0, 'CCP_NG_169': 0, 'CCP_78': 1, 'CCP_523': 0, 'CCP_NG_117': 1, 'CCP_NG_177': 1, 'CCP_266': 0, 'CCP_28': 1, 'CCP_NG_174': 1, 'CCP_516': 0, 'CCP_646': 0, 'CCP_NG_29': 0, 'CCP_NG_59': 1, 'CCP_169': 1, 'CCP_695': 1, 'CCP_290': 1, 'CCP_NG_17': 1, 'CCP_NG_185': 1, 'CCP_352': 0, 'CCP_87': 0, 'CCP_304': 0, 'CCP_57': 1, 'CCP_420': 1, 'CCP_107': 0, 'CCP_82': 1, 'CCP_1001': 1, 'CCP_65': 0, 'CCP_NG_54': 1, 'CCP_1008': 1, 'CCP_519': 1}\n",
      "{'CCP_947': 1, 'CCP_402': 1, 'CCP_1018': 0, 'CCP_NG_100': 0, 'CCP_180': 0, 'CCP_NG_1': 0, 'CCP_565': 1, 'CCP_612': 0, 'CCP_NG_23': 1, 'CCP_153': 0, 'CCP_804': 0, 'CCP_NG_67': 0, 'CCP_371': 1, 'CCP_NG_77': 0, 'CCP_511': 1, 'CCP_444': 1, 'CCP_70': 1, 'CCP_NG_229': 0, 'CCP_279': 1, 'CCP_239': 0, 'CCP_56': 1, 'CCP_NG_143': 0, 'CCP_829': 0, 'CCP_NG_197': 1, 'CCP_NG_113': 0, 'CCP_168': 0, 'CCP_43': 0, 'CCP_380': 1, 'CCP_NG_122': 0, 'CCP_369': 1, 'CCP_94': 0, 'CCP_46': 0, 'CCP_214': 1, 'CCP_954': 0, 'CCP_510': 0, 'CCP_113': 1, 'CCP_969': 1, 'CCP_45': 1, 'CCP_NG_39': 0, 'CCP_508': 0, 'CCP_823': 1, 'CCP_349': 0, 'CCP_598': 1, 'CCP_846': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe3f30d69af48169623bcc545db7800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best instance number: 57\n",
      "Patient ID: CCP_105, Image shape: (96, 384, 512)\n",
      "Best instance number: 109\n",
      "Patient ID: CCP_780, Image shape: (96, 384, 512)\n",
      "Best instance number: 67\n",
      "Patient ID: CCP_531, Image shape: (96, 384, 512)\n",
      "Best instance number: 124\n",
      "Patient ID: CCP_672, Image shape: (96, 384, 512)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 96, 96, 96], got [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m MaskedAutoencoder3D(in_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train(model, train_loader, valid_loader, optimizer, epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, results_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m early_stopper \u001b[39m=\u001b[39m EarlyStopper(patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m0.03\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step(model, train_dataloader, loss_fn, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m val_step(model, val_dataloader, loss_fn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mif\u001b[39;00m early_stopper(val_loss):\n",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m mask \u001b[39m=\u001b[39m create_mask(images\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images, mask)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mcross_entropy(\u001b[39minput\u001b[39m, target, weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mcross_entropy_loss(\u001b[39minput\u001b[39m, target, weight, _Reduction\u001b[39m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 96, 96, 96], got [4]"
     ]
    }
   ],
   "source": [
    " if __name__ == '__main__':\n",
    "    # Assuming 'HandScanDataset2' and 'MaskedAutoencoder3D' are defined elsewhere\n",
    "    training_data_dir = \"/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/\"\n",
    "    csv_path = os.path.join(training_data_dir, 'training_labels_subset.csv')\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "    train_df, valid_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['progression'])\n",
    "    train_df.to_csv(os.path.join(training_data_dir, 'train_split.csv'), index=False)\n",
    "    valid_df.to_csv(os.path.join(training_data_dir, 'valid_split.csv'), index=False)\n",
    "    \n",
    "\n",
    "    train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "    valid_dataset = HandScanDataset2(labels_df=valid_df, data_dir=training_data_dir, transform=validation_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MaskedAutoencoder3D(in_channels=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train(model, train_loader, valid_loader, optimizer, epochs=50, results_path=\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CCP_120': 1, 'CCP_117': 0, 'CCP_73': 0, 'CCP_386': 1, 'CCP_644': 1, 'CCP_NG_166': 0, 'CCP_NG_42': 1, 'CCP_138': 0, 'CCP_874': 0, 'CCP_907': 1, 'CCP_NG_137': 1, 'CCP_647': 1, 'CCP_262': 0, 'CCP_541': 0, 'CCP_89': 0, 'CCP_202': 0, 'CCP_NG_56': 0, 'CCP_821': 1, 'CCP_66': 0, 'CCP_471': 0, 'CCP_34': 0, 'CCP_181': 1, 'CCP_557': 1, 'CCP_416': 0, 'CCP_NG_207': 0, 'CCP_568': 0, 'CCP_753': 0, 'CCP_NG_181': 0, 'CCP_81': 0, 'CCP_415': 1, 'CCP_NG_8': 1, 'CCP_283': 1, 'CCP_906': 0, 'CCP_968': 0, 'CCP_664': 0, 'CCP_736': 0, 'CCP_355': 0, 'CCP_NG_104': 1, 'CCP_247': 1, 'CCP_NG_188': 1, 'CCP_976': 0, 'CCP_NG_214': 0, 'CCP_824': 1, 'CCP_62': 0, 'CCP_NG_36': 0, 'CCP_802': 0, 'CCP_NG_172': 0, 'CCP_873': 0, 'CCP_207': 0, 'CCP_167': 1, 'CCP_944': 1, 'CCP_133': 1, 'CCP_NG_60': 0, 'CCP_1000': 0, 'CCP_252': 1, 'CCP_672': 0, 'CCP_531': 0, 'CCP_NG_175': 1, 'CCP_NG_107': 1, 'CCP_53': 0, 'CCP_NG_106': 1, 'CCP_105': 0, 'CCP_507': 1, 'CCP_405': 0, 'CCP_172': 1, 'CCP_901': 1, 'CCP_212': 1, 'CCP_485': 0, 'CCP_185': 0, 'CCP_422': 0, 'CCP_245': 1, 'CCP_NG_86': 0, 'CCP_263': 0, 'CCP_NG_150': 0, 'CCP_354': 1, 'CCP_307': 0, 'CCP_668': 1, 'CCP_174': 0, 'CCP_NG_16': 0, 'CCP_NG_49': 0, 'CCP_505': 0, 'CCP_849': 1, 'CCP_573': 0, 'CCP_643': 0, 'CCP_47': 0, 'CCP_619': 1, 'CCP_NG_52': 0, 'CCP_879': 1, 'CCP_102': 1, 'CCP_752': 0, 'CCP_NG_116': 0, 'CCP_NG_85': 1, 'CCP_457': 1, 'CCP_NG_79': 1, 'CCP_229': 0, 'CCP_199': 0, 'CCP_NG_147': 1, 'CCP_828': 0, 'CCP_131': 1, 'CCP_NG_144': 0, 'CCP_794': 1, 'CCP_NG_178': 0, 'CCP_783': 1, 'CCP_100': 0, 'CCP_330': 1, 'CCP_859': 0, 'CCP_657': 1, 'CCP_393': 0, 'CCP_NG_102': 1, 'CCP_635': 1, 'CCP_426': 0, 'CCP_NG_9': 0, 'CCP_520': 0, 'CCP_780': 1, 'CCP_894': 0, 'CCP_412': 0, 'CCP_50': 1, 'CCP_NG_96': 0, 'CCP_NG_160': 1, 'CCP_638': 0, 'CCP_124': 1, 'CCP_864': 0, 'CCP_103': 1, 'CCP_NG_97': 1, 'CCP_321': 0, 'CCP_228': 1, 'CCP_631': 1, 'CCP_666': 0, 'CCP_71': 0, 'CCP_319': 0, 'CCP_616': 1, 'CCP_52': 0, 'CCP_191': 1, 'CCP_NG_68': 0, 'CCP_NG_140': 1, 'CCP_44': 1, 'CCP_827': 0, 'CCP_830': 1, 'CCP_419': 0, 'CCP_389': 0, 'CCP_909': 1, 'CCP_414': 1, 'CCP_884': 0, 'CCP_NG_169': 0, 'CCP_78': 1, 'CCP_523': 0, 'CCP_NG_117': 1, 'CCP_NG_177': 1, 'CCP_266': 0, 'CCP_28': 1, 'CCP_NG_174': 1, 'CCP_516': 0, 'CCP_646': 0, 'CCP_NG_29': 0, 'CCP_NG_59': 1, 'CCP_169': 1, 'CCP_695': 1, 'CCP_290': 1, 'CCP_NG_17': 1, 'CCP_NG_185': 1, 'CCP_352': 0, 'CCP_87': 0, 'CCP_304': 0, 'CCP_57': 1, 'CCP_420': 1, 'CCP_107': 0, 'CCP_82': 1, 'CCP_1001': 1, 'CCP_65': 0, 'CCP_NG_54': 1, 'CCP_1008': 1, 'CCP_519': 1}\n",
      "{'CCP_947': 1, 'CCP_402': 1, 'CCP_1018': 0, 'CCP_NG_100': 0, 'CCP_180': 0, 'CCP_NG_1': 0, 'CCP_565': 1, 'CCP_612': 0, 'CCP_NG_23': 1, 'CCP_153': 0, 'CCP_804': 0, 'CCP_NG_67': 0, 'CCP_371': 1, 'CCP_NG_77': 0, 'CCP_511': 1, 'CCP_444': 1, 'CCP_70': 1, 'CCP_NG_229': 0, 'CCP_279': 1, 'CCP_239': 0, 'CCP_56': 1, 'CCP_NG_143': 0, 'CCP_829': 0, 'CCP_NG_197': 1, 'CCP_NG_113': 0, 'CCP_168': 0, 'CCP_43': 0, 'CCP_380': 1, 'CCP_NG_122': 0, 'CCP_369': 1, 'CCP_94': 0, 'CCP_46': 0, 'CCP_214': 1, 'CCP_954': 0, 'CCP_510': 0, 'CCP_113': 1, 'CCP_969': 1, 'CCP_45': 1, 'CCP_NG_39': 0, 'CCP_508': 0, 'CCP_823': 1, 'CCP_349': 0, 'CCP_598': 1, 'CCP_846': 1}\n",
      "Best instance number: 68\n",
      "Patient ID: CCP_65, Image shape: (96, 384, 512)\n",
      "Best instance number: 86\n",
      "Patient ID: CCP_821, Image shape: (96, 384, 512)\n",
      "Best instance number: 72\n",
      "Patient ID: CCP_422, Image shape: (96, 384, 512)\n",
      "Best instance number: 60\n",
      "Patient ID: CCP_NG_140, Image shape: (96, 384, 512)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CrossEntropyLoss' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m valid_loader \u001b[39m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model \u001b[39m=\u001b[39m MaskedAutoencoder3D(in_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_mae(model, train_loader, valid_loader, num_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossEntropyLoss' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Reading the CSV file\n",
    "    training_data_dir = \"/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/\" \n",
    "    csv_path = os.path.join(training_data_dir, 'training_labels_subset.csv')\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_df, valid_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['progression'])\n",
    "\n",
    "    # Save the splits for reference\n",
    "    train_df.to_csv(os.path.join(training_data_dir, 'train_split.csv'), index=False)\n",
    "    valid_df.to_csv(os.path.join(training_data_dir, 'valid_split.csv'), index=False)\n",
    "    train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "    valid_dataset = HandScanDataset2(labels_df=valid_df, data_dir=training_data_dir, transform=validation_transform)\n",
    "\n",
    "    # Creating data loaders\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = MaskedAutoencoder3D(in_channels=1)\n",
    "    train_mae(model, train_loader, valid_loader, num_epochs=50, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
