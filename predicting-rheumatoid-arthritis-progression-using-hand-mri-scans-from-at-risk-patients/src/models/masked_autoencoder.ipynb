{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre_processing \n",
    "from pre_processing import HandScanDataset2, transform, validation_transform, train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvBlock3D, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Encoder3D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            ConvBlock3D(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            ConvBlock3D(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            ConvBlock3D(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Decoder3D, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, 128, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder3D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(MaskedAutoencoder3D, self).__init__()\n",
    "        self.encoder = Encoder3D(in_channels)\n",
    "        self.decoder = Decoder3D(256)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_masked = x * mask\n",
    "        encoded = self.encoder(x_masked)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(shape, mask_ratio=0.7):\n",
    "    mask = np.ones(shape)\n",
    "    num_elements = shape[1] * shape[2] * shape[3]\n",
    "    num_masked = int(mask_ratio * num_elements)\n",
    "    indices = np.random.choice(num_elements, num_masked, replace=False)\n",
    "    mask.reshape(-1)[indices] = 0\n",
    "    return torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mae(model, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            mask = create_mask(images.shape)\n",
    "            outputs = model(images, mask)\n",
    "            loss = custom_loss(outputs, images)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                mask = create_mask(images.shape)\n",
    "                outputs = model(images, mask)\n",
    "                loss = custom_loss(outputs, images)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CCP_120': 1, 'CCP_117': 0, 'CCP_73': 0, 'CCP_386': 1, 'CCP_644': 1, 'CCP_NG_166': 0, 'CCP_NG_42': 1, 'CCP_138': 0, 'CCP_874': 0, 'CCP_907': 1, 'CCP_NG_137': 1, 'CCP_647': 1, 'CCP_262': 0, 'CCP_541': 0, 'CCP_89': 0, 'CCP_202': 0, 'CCP_NG_56': 0, 'CCP_821': 1, 'CCP_66': 0, 'CCP_471': 0, 'CCP_34': 0, 'CCP_181': 1, 'CCP_557': 1, 'CCP_416': 0, 'CCP_NG_207': 0, 'CCP_568': 0, 'CCP_753': 0, 'CCP_NG_181': 0, 'CCP_81': 0, 'CCP_415': 1, 'CCP_NG_8': 1, 'CCP_283': 1, 'CCP_906': 0, 'CCP_968': 0, 'CCP_664': 0, 'CCP_736': 0, 'CCP_355': 0, 'CCP_NG_104': 1, 'CCP_247': 1, 'CCP_NG_188': 1, 'CCP_976': 0, 'CCP_NG_214': 0, 'CCP_824': 1, 'CCP_62': 0, 'CCP_NG_36': 0, 'CCP_802': 0, 'CCP_NG_172': 0, 'CCP_873': 0, 'CCP_207': 0, 'CCP_167': 1, 'CCP_944': 1, 'CCP_133': 1, 'CCP_NG_60': 0, 'CCP_1000': 0, 'CCP_252': 1, 'CCP_672': 0, 'CCP_531': 0, 'CCP_NG_175': 1, 'CCP_NG_107': 1, 'CCP_53': 0, 'CCP_NG_106': 1, 'CCP_105': 0, 'CCP_507': 1, 'CCP_405': 0, 'CCP_172': 1, 'CCP_901': 1, 'CCP_212': 1, 'CCP_485': 0, 'CCP_185': 0, 'CCP_422': 0, 'CCP_245': 1, 'CCP_NG_86': 0, 'CCP_263': 0, 'CCP_NG_150': 0, 'CCP_354': 1, 'CCP_307': 0, 'CCP_668': 1, 'CCP_174': 0, 'CCP_NG_16': 0, 'CCP_NG_49': 0, 'CCP_505': 0, 'CCP_849': 1, 'CCP_573': 0, 'CCP_643': 0, 'CCP_47': 0, 'CCP_619': 1, 'CCP_NG_52': 0, 'CCP_879': 1, 'CCP_102': 1, 'CCP_752': 0, 'CCP_NG_116': 0, 'CCP_NG_85': 1, 'CCP_457': 1, 'CCP_NG_79': 1, 'CCP_229': 0, 'CCP_199': 0, 'CCP_NG_147': 1, 'CCP_828': 0, 'CCP_131': 1, 'CCP_NG_144': 0, 'CCP_794': 1, 'CCP_NG_178': 0, 'CCP_783': 1, 'CCP_100': 0, 'CCP_330': 1, 'CCP_859': 0, 'CCP_657': 1, 'CCP_393': 0, 'CCP_NG_102': 1, 'CCP_635': 1, 'CCP_426': 0, 'CCP_NG_9': 0, 'CCP_520': 0, 'CCP_780': 1, 'CCP_894': 0, 'CCP_412': 0, 'CCP_50': 1, 'CCP_NG_96': 0, 'CCP_NG_160': 1, 'CCP_638': 0, 'CCP_124': 1, 'CCP_864': 0, 'CCP_103': 1, 'CCP_NG_97': 1, 'CCP_321': 0, 'CCP_228': 1, 'CCP_631': 1, 'CCP_666': 0, 'CCP_71': 0, 'CCP_319': 0, 'CCP_616': 1, 'CCP_52': 0, 'CCP_191': 1, 'CCP_NG_68': 0, 'CCP_NG_140': 1, 'CCP_44': 1, 'CCP_827': 0, 'CCP_830': 1, 'CCP_419': 0, 'CCP_389': 0, 'CCP_909': 1, 'CCP_414': 1, 'CCP_884': 0, 'CCP_NG_169': 0, 'CCP_78': 1, 'CCP_523': 0, 'CCP_NG_117': 1, 'CCP_NG_177': 1, 'CCP_266': 0, 'CCP_28': 1, 'CCP_NG_174': 1, 'CCP_516': 0, 'CCP_646': 0, 'CCP_NG_29': 0, 'CCP_NG_59': 1, 'CCP_169': 1, 'CCP_695': 1, 'CCP_290': 1, 'CCP_NG_17': 1, 'CCP_NG_185': 1, 'CCP_352': 0, 'CCP_87': 0, 'CCP_304': 0, 'CCP_57': 1, 'CCP_420': 1, 'CCP_107': 0, 'CCP_82': 1, 'CCP_1001': 1, 'CCP_65': 0, 'CCP_NG_54': 1, 'CCP_1008': 1, 'CCP_519': 1}\n",
      "{'CCP_947': 1, 'CCP_402': 1, 'CCP_1018': 0, 'CCP_NG_100': 0, 'CCP_180': 0, 'CCP_NG_1': 0, 'CCP_565': 1, 'CCP_612': 0, 'CCP_NG_23': 1, 'CCP_153': 0, 'CCP_804': 0, 'CCP_NG_67': 0, 'CCP_371': 1, 'CCP_NG_77': 0, 'CCP_511': 1, 'CCP_444': 1, 'CCP_70': 1, 'CCP_NG_229': 0, 'CCP_279': 1, 'CCP_239': 0, 'CCP_56': 1, 'CCP_NG_143': 0, 'CCP_829': 0, 'CCP_NG_197': 1, 'CCP_NG_113': 0, 'CCP_168': 0, 'CCP_43': 0, 'CCP_380': 1, 'CCP_NG_122': 0, 'CCP_369': 1, 'CCP_94': 0, 'CCP_46': 0, 'CCP_214': 1, 'CCP_954': 0, 'CCP_510': 0, 'CCP_113': 1, 'CCP_969': 1, 'CCP_45': 1, 'CCP_NG_39': 0, 'CCP_508': 0, 'CCP_823': 1, 'CCP_349': 0, 'CCP_598': 1, 'CCP_846': 1}\n",
      "Best instance number: 69\n",
      "Patient ID: CCP_NG_137, Image shape: (96, 384, 512)\n",
      "Best instance number: 78\n",
      "Patient ID: CCP_523, Image shape: (96, 384, 512)\n",
      "Best instance number: 101\n",
      "Patient ID: CCP_619, Image shape: (96, 384, 512)\n",
      "Best instance number: 67\n",
      "Patient ID: CCP_NG_97, Image shape: (96, 384, 512)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"mse_cpu\" not implemented for 'ComplexFloat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m valid_loader \u001b[39m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model \u001b[39m=\u001b[39m MaskedAutoencoder3D(in_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_mae(model, train_loader, valid_loader, num_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mask \u001b[39m=\u001b[39m create_mask(images\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images, mask)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m custom_loss(outputs, images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mse_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pixel_loss \u001b[39m=\u001b[39m mse_loss(output, target)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m freq_loss \u001b[39m=\u001b[39m mse_loss(torch\u001b[39m.\u001b[39mfft\u001b[39m.\u001b[39mfftn(output), torch\u001b[39m.\u001b[39mfft\u001b[39m.\u001b[39mfftn(target))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/masked_autoencoder.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m alpha \u001b[39m*\u001b[39m pixel_loss \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha) \u001b[39m*\u001b[39m freq_loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmse_loss(\u001b[39minput\u001b[39m, target, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3329\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3326\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3328\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[0;32m-> 3329\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"mse_cpu\" not implemented for 'ComplexFloat'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Reading the CSV file\n",
    "    training_data_dir = \"/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/\" \n",
    "    csv_path = os.path.join(training_data_dir, 'training_labels_subset.csv')\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_df, valid_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['progression'])\n",
    "\n",
    "    # Save the splits for reference\n",
    "    train_df.to_csv(os.path.join(training_data_dir, 'train_split.csv'), index=False)\n",
    "    valid_df.to_csv(os.path.join(training_data_dir, 'valid_split.csv'), index=False)\n",
    "    train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "    valid_dataset = HandScanDataset2(labels_df=valid_df, data_dir=training_data_dir, transform=validation_transform)\n",
    "\n",
    "    # Creating data loaders\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = MaskedAutoencoder3D(in_channels=1)\n",
    "    train_mae(model, train_loader, valid_loader, num_epochs=50, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
