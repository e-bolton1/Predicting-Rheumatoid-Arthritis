{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/CCP_MRI_image_subset\n",
      "Sample 0: Image shape: torch.Size([1, 20, 512, 512]), Label: 1\n",
      "Sample 1: Image shape: torch.Size([1, 20, 512, 512]), Label: 0\n",
      "Sample 2: Image shape: torch.Size([1, 20, 512, 512]), Label: 0\n",
      "Batch image shape: torch.Size([1, 1, 20, 512, 512])\n",
      "Batch label shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from pre_processing_py import HandScanDataset2, transform, validation_transform, train_df, valid_df, training_data_dir, display_images\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=4, embed_dim=64):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, D/patch_size, H/patch_size, W/patch_size)\n",
    "        x = rearrange(x, 'b c d h w -> b (d h w) c')  # Flatten to (B, N, embed_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, mlp_ratio=4.0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x, x, x)[0])\n",
    "        x = self.norm2(x + self.mlp(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisruptiveAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=4, embed_dim=64, depth=4, num_heads=4, mlp_ratio=4.0):\n",
    "        super(DisruptiveAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)\n",
    "        ])\n",
    "        self.reconstruction_head = nn.Linear(embed_dim, patch_size ** 3 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape  # Save the original shape\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Encoder\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Decoder\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Reconstruction\n",
    "        x = self.reconstruction_head(x)\n",
    "        \n",
    "        # Reshape back to original dimensions before applying L1 loss\n",
    "        # Assume d_patches=2, h_patches=64, w_patches=64 (or calculate as needed)\n",
    "        d_patches, h_patches, w_patches = 2, 64, 64\n",
    "        x = rearrange(x, 'b (d h w) (p1 p2 p3 c) -> b c (d p1) (h p2) (w p3)', d=d_patches, h=h_patches, w=w_patches, p1=4, p2=4, p3=4)\n",
    "        \n",
    "        # Upsample to the original shape\n",
    "        x = F.interpolate(x, size=original_shape[2:], mode='trilinear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def add_noise(self, x, noise_level=0.1):\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        return x + noise\n",
    "\n",
    "    def downsample(self, x, scale_factor=0.5):\n",
    "        return F.interpolate(x, scale_factor=scale_factor, mode='trilinear', align_corners=False)\n",
    "\n",
    "    def local_mask(self, x, mask_ratio=0.15):\n",
    "        \"\"\"\n",
    "        Apply local masking by setting a percentage of channels to zero.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor with shape [batch_size, channels, depth, height, width].\n",
    "            mask_ratio: Ratio of channels to be masked.\n",
    "\n",
    "        Returns:\n",
    "            Masked tensor with the same shape as input.\n",
    "        \"\"\"\n",
    "        # Determine the shape of the input tensor\n",
    "        b, c, d, h, w = x.shape  # Assume x is [batch_size, channels, depth, height, width]\n",
    "        \n",
    "        num_masked = int(mask_ratio * c)\n",
    "        mask_indices = torch.randperm(c)[:num_masked]  # Randomly select indices of channels to mask\n",
    "\n",
    "        x[:, mask_indices, :, :, :] = 0  # Mask the selected channels\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, reconstructed, original, zsim, zlabel, alpha=0.05):\n",
    "        # Ensure the reconstructed output matches the original size\n",
    "        reconstructed = F.interpolate(reconstructed, size=original.shape[2:], mode='trilinear', align_corners=False)\n",
    "\n",
    "        # L1 reconstruction loss\n",
    "        l1_loss = F.l1_loss(reconstructed, original)\n",
    "\n",
    "        # Contrastive loss (LCMCL)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(zsim, zlabel)\n",
    "        contrastive_loss = alpha * bce_loss\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = l1_loss + contrastive_loss\n",
    "        return total_loss, l1_loss, contrastive_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "# Initialize dataset and data loader\n",
    "train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few subjects (e.g., the first three subjects)\n",
    "test_subjects_df = train_df.iloc[:3]\n",
    "\n",
    "# Initialize the dataset with the selected subjects\n",
    "test_dataset = HandScanDataset2(labels_df=test_subjects_df, data_dir=training_data_dir, transform=transform)\n",
    "\n",
    "# Create a data loader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [1/10], Loss: 0.1328, L1 Loss: 7.6430, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [2/10], Loss: 0.1301, L1 Loss: 8.0778, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [3/10], Loss: 0.1301, L1 Loss: 7.7631, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [4/10], Loss: 0.1286, L1 Loss: 8.1459, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [5/10], Loss: 0.1280, L1 Loss: 7.8765, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [6/10], Loss: 0.1297, L1 Loss: 7.8964, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [7/10], Loss: 0.1282, L1 Loss: 7.8927, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [8/10], Loss: 0.1262, L1 Loss: 7.8524, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [9/10], Loss: 0.1269, L1 Loss: 7.8858, Contrastive Loss: 0.0000\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Shape before local masking: torch.Size([1, 1, 10, 256, 256])\n",
      "Epoch [10/10], Loss: 0.1267, L1 Loss: 7.8053, Contrastive Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the DisruptiveAutoencoder model (this should be done before the training loop)\n",
    "dae_model = DisruptiveAutoencoder(\n",
    "    in_channels=1,       # Input channel dimension, typically 1 for grayscale medical images\n",
    "    patch_size=4,        # Size of each patch\n",
    "    embed_dim=64,        # Embedding dimension size\n",
    "    depth=4,             # Number of transformer layers\n",
    "    num_heads=4,         # Number of attention heads\n",
    "    mlp_ratio=4.0        # Ratio of MLP hidden layer dimension to embedding dimension\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dae_model = dae_model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(dae_model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    dae_model.train()  # Set model to training mode\n",
    "    epoch_loss = 0\n",
    "    for patches, labels in test_loader:\n",
    "        patches = patches.to(device)  # Move patches to the same device as the model\n",
    "        labels = labels.to(device)    # Move labels to the same device as the model\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply noise, downsampling, and local masking\n",
    "        noisy_patches = dae_model.add_noise(patches)\n",
    "        downsampled_patches = dae_model.downsample(noisy_patches)\n",
    "        \n",
    "        # Print shape for debugging\n",
    "        print(\"Shape before local masking:\", downsampled_patches.shape)\n",
    "        \n",
    "        masked_patches = dae_model.local_mask(downsampled_patches)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed_patches = dae_model(masked_patches)\n",
    "\n",
    "        # Generate zsim and zlabel for contrastive learning\n",
    "        zsim = torch.mm(reconstructed_patches.view(reconstructed_patches.size(0), -1), \n",
    "                        reconstructed_patches.view(reconstructed_patches.size(0), -1).T)\n",
    "        zlabel = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()  # Binary label matrix for positive pairs\n",
    "\n",
    "        # Compute combined loss\n",
    "        total_loss, l1_loss, contrastive_loss = dae_model.compute_loss(reconstructed_patches, patches, zsim, zlabel)\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, '\n",
    "          f'L1 Loss: {l1_loss.item():.4f}, Contrastive Loss: {contrastive_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
