{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/CCP_MRI_image_subset\n",
      "Sample 0: Image shape: torch.Size([1, 20, 512, 512]), Label: 1\n",
      "Sample 1: Image shape: torch.Size([1, 20, 512, 512]), Label: 0\n",
      "Sample 2: Image shape: torch.Size([1, 20, 512, 512]), Label: 0\n",
      "Batch image shape: torch.Size([1, 1, 20, 512, 512])\n",
      "Batch label shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from pre_processing_py import HandScanDataset2, transform, validation_transform, train_df, valid_df, training_data_dir, display_images\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=4, embed_dim=64):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, D/patch_size, H/patch_size, W/patch_size)\n",
    "        x = rearrange(x, 'b c d h w -> b (d h w) c')  # Flatten to (B, N, embed_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, mlp_ratio=4.0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x, x, x)[0])\n",
    "        x = self.norm2(x + self.mlp(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisruptiveAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=4, embed_dim=64, depth=4, num_heads=4, mlp_ratio=4.0):\n",
    "        super(DisruptiveAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)\n",
    "        ])\n",
    "        self.reconstruction_head = nn.Linear(embed_dim, patch_size ** 3 * in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape  # Save the original shape\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Encoder\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Decoder\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Reconstruction\n",
    "        x = self.reconstruction_head(x)\n",
    "        \n",
    "        # Reshape back to original dimensions before applying L1 loss\n",
    "        # Assume d_patches=2, h_patches=64, w_patches=64 (or calculate as needed)\n",
    "        d_patches, h_patches, w_patches = 2, 64, 64\n",
    "        x = rearrange(x, 'b (d h w) (p1 p2 p3 c) -> b c (d p1) (h p2) (w p3)', d=d_patches, h=h_patches, w=w_patches, p1=4, p2=4, p3=4)\n",
    "        \n",
    "        # Upsample to the original shape\n",
    "        x = F.interpolate(x, size=original_shape[2:], mode='trilinear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def add_noise(self, x, noise_level=0.1):\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        return x + noise\n",
    "\n",
    "    def downsample(self, x, scale_factor=0.5):\n",
    "        return F.interpolate(x, scale_factor=scale_factor, mode='trilinear', align_corners=False)\n",
    "\n",
    "    def local_mask(self, x, mask_ratio=0.15):\n",
    "        \"\"\"\n",
    "        Apply local masking by setting a percentage of channels to zero.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor with shape [batch_size, channels, depth, height, width].\n",
    "            mask_ratio: Ratio of channels to be masked.\n",
    "\n",
    "        Returns:\n",
    "            Masked tensor with the same shape as input.\n",
    "        \"\"\"\n",
    "        # Determine the shape of the input tensor\n",
    "        b, c, d, h, w = x.shape  # Assume x is [batch_size, channels, depth, height, width]\n",
    "        \n",
    "        num_masked = int(mask_ratio * c)\n",
    "        mask_indices = torch.randperm(c)[:num_masked]  # Randomly select indices of channels to mask\n",
    "\n",
    "        x[:, mask_indices, :, :, :] = 0  # Mask the selected channels\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, reconstructed, original, zsim, zlabel, alpha=0.05):\n",
    "        # Ensure the reconstructed output matches the original size\n",
    "        reconstructed = F.interpolate(reconstructed, size=original.shape[2:], mode='trilinear', align_corners=False)\n",
    "\n",
    "        # L1 reconstruction loss\n",
    "        l1_loss = F.l1_loss(reconstructed, original)\n",
    "\n",
    "        # Contrastive loss (LCMCL)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(zsim, zlabel)\n",
    "        contrastive_loss = alpha * bce_loss\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = l1_loss + contrastive_loss\n",
    "        return total_loss, l1_loss, contrastive_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "# Initialize dataset and data loader\n",
    "train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset with the selected subjects\n",
    "valid_dataset = HandScanDataset2(labels_df=valid_df, data_dir=training_data_dir, transform=validation_transform)\n",
    "# Create a data loader for testing\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few subjects (e.g., the first three subjects)\n",
    "test_subjects_df = train_df.iloc[:3]\n",
    "\n",
    "# Initialize the dataset with the selected subjects\n",
    "test_dataset = HandScanDataset2(labels_df=test_subjects_df, data_dir=training_data_dir, transform=transform)\n",
    "\n",
    "# Create a data loader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.activation_count = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.activation_count += 1\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "def visualize_reconstruction(dae_model, patches, idx, device, num_slices=1):\n",
    "    \"\"\"\n",
    "    Visualizes the original, tokenized patches, and reconstructed images.\n",
    "\n",
    "    Args:\n",
    "        dae_model (nn.Module): The trained autoencoder model.\n",
    "        patches (torch.Tensor): The input patches.\n",
    "        idx (int): Index of the image (used for title).\n",
    "        device (torch.device): The device to which the model and data are moved.\n",
    "        num_slices (int): Number of slices to visualize.\n",
    "    \"\"\"\n",
    "    dae_model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Move patches to the appropriate device\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get the intermediate and final outputs\n",
    "        noisy_patches = dae_model.add_noise(patches)\n",
    "        downsampled_patches = dae_model.downsample(noisy_patches)\n",
    "        tokenized_patches = dae_model.patch_embed(downsampled_patches)  # Tokenization step\n",
    "        masked_patches = dae_model.local_mask(downsampled_patches)\n",
    "        reconstructed_patches = dae_model(masked_patches)\n",
    "\n",
    "    # Convert tensors to numpy arrays for visualization\n",
    "    original_image = patches.cpu().numpy()[0, 0, :, :, :]  # Assuming single-channel, 3D volume\n",
    "    reconstructed_image = reconstructed_patches.cpu().numpy()[0, 0, :, :, :]\n",
    "\n",
    "    # Reconstruct patches into a grid to visualize\n",
    "    patch_grid = rearrange(tokenized_patches.cpu().numpy()[0], '(d h w) c -> d h w c', d=2, h=64, w=64)\n",
    "    patch_grid_image = np.mean(patch_grid, axis=-1)[0]  # Visualizing the mean across the channels\n",
    "\n",
    "    # Display the images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for slice_idx in range(min(num_slices, original_image.shape[0])):\n",
    "        plt.subplot(3, num_slices, slice_idx + 1)\n",
    "        plt.imshow(original_image[slice_idx], cmap='gray')\n",
    "        plt.title(f'Original Image {idx} - Slice {slice_idx}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(3, num_slices, num_slices + slice_idx + 1)\n",
    "        plt.imshow(patch_grid_image, cmap='gray')\n",
    "        plt.title(f'Patches {idx} - Slice {slice_idx}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(3, num_slices, 2 * num_slices + slice_idx + 1)\n",
    "        plt.imshow(reconstructed_image[slice_idx], cmap='gray')\n",
    "        plt.title(f'Reconstructed {idx} - Slice {slice_idx}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [1/10], Validation Loss: 7.5646\n",
      "Epoch [1/10], Loss: 6.6615, L1 Loss: 8.2956, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [2/10], Validation Loss: 7.7013\n",
      "Epoch [2/10], Loss: 5.6154, L1 Loss: 8.4265, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [3/10], Validation Loss: 7.7581\n",
      "Epoch [3/10], Loss: 5.0407, L1 Loss: 8.4756, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [4/10], Validation Loss: 7.3953\n",
      "Epoch [4/10], Loss: 6.6239, L1 Loss: 8.1288, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [5/10], Validation Loss: 7.4017\n",
      "Epoch [5/10], Loss: 7.2800, L1 Loss: 8.1353, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [6/10], Validation Loss: 7.4145\n",
      "Epoch [6/10], Loss: 7.2740, L1 Loss: 8.1480, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [7/10], Validation Loss: 7.3901\n",
      "Epoch [7/10], Loss: 7.2703, L1 Loss: 8.1236, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [8/10], Validation Loss: 7.3990\n",
      "Epoch [8/10], Loss: 7.2801, L1 Loss: 8.1325, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [9/10], Validation Loss: 7.3941\n",
      "Epoch [9/10], Loss: 7.2706, L1 Loss: 8.1276, Contrastive Loss: 0.0000\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_181/983862e0ae/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.201309201106223154520723.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Error reading /Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/t1_vibe_we_hand_subset/CCP_393/f6a3f98dc1/t1_vibe_we/1.3.12.2.1107.5.2.36.40258.2016030308584524314805431.DCM: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading.\n",
      "Epoch [10/10], Validation Loss: 8.0770\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the DisruptiveAutoencoder model (this should be done before the training loop)\n",
    "dae_model = DisruptiveAutoencoder(\n",
    "    in_channels=1,       # Input channel dimension, typically 1 for grayscale medical images\n",
    "    patch_size=4,        # Size of each patch\n",
    "    embed_dim=64,        # Embedding dimension size\n",
    "    depth=4,             # Number of transformer layers\n",
    "    num_heads=4,         # Number of attention heads\n",
    "    mlp_ratio=4.0        # Ratio of MLP hidden layer dimension to embedding dimension\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dae_model = dae_model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(dae_model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the EarlyStopper\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    dae_model.train()  # Set model to training mode\n",
    "    epoch_loss = 0\n",
    "    for patches, labels in train_loader:\n",
    "        patches = patches.to(device)  # Move patches to the same device as the model\n",
    "        labels = labels.to(device)    # Move labels to the same device as the model\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply noise, downsampling, and local masking\n",
    "        noisy_patches = dae_model.add_noise(patches)\n",
    "        downsampled_patches = dae_model.downsample(noisy_patches)\n",
    "        \n",
    "        # Forward pass\n",
    "        masked_patches = dae_model.local_mask(downsampled_patches)\n",
    "        reconstructed_patches = dae_model(masked_patches)\n",
    "\n",
    "        # Generate zsim and zlabel for contrastive learning\n",
    "        zsim = torch.mm(reconstructed_patches.view(reconstructed_patches.size(0), -1), \n",
    "                        reconstructed_patches.view(reconstructed_patches.size(0), -1).T)\n",
    "        zlabel = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()  # Binary label matrix for positive pairs\n",
    "\n",
    "        # Compute combined loss\n",
    "        total_loss, l1_loss, contrastive_loss = dae_model.compute_loss(reconstructed_patches, patches, zsim, zlabel)\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase (or calculate validation loss)\n",
    "    dae_model.eval()  # Set model to evaluation mode\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for patches, labels in valid_loader:\n",
    "            patches = patches.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            noisy_patches = dae_model.add_noise(patches)\n",
    "            downsampled_patches = dae_model.downsample(noisy_patches)\n",
    "            masked_patches = dae_model.local_mask(downsampled_patches)\n",
    "            reconstructed_patches = dae_model(masked_patches)\n",
    "\n",
    "            # Compute loss\n",
    "            zsim = torch.mm(reconstructed_patches.view(reconstructed_patches.size(0), -1), \n",
    "                            reconstructed_patches.view(reconstructed_patches.size(0), -1).T)\n",
    "            zlabel = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "\n",
    "            _, l1_loss, contrastive_loss = dae_model.compute_loss(reconstructed_patches, patches, zsim, zlabel)\n",
    "            validation_loss += l1_loss.item()\n",
    "\n",
    "    # Average validation loss\n",
    "    validation_loss /= len(valid_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {validation_loss:.4f}')\n",
    "\n",
    "    # Check early stopping\n",
    "    if early_stopper.early_stop(validation_loss):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, '\n",
    "          f'L1 Loss: {l1_loss.item():.4f}, Contrastive Loss: {contrastive_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
