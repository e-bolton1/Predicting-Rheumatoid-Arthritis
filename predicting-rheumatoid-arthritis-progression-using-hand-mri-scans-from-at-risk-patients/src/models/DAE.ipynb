{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eleanorbolton/Library/CloudStorage/OneDrive-UniversityofLeeds/CCP_MRI_image_subset\n",
      "Sample 0: Image shape: torch.Size([1, 20, 512, 512]), Label: 1\n",
      "Sample 1: Image shape: torch.Size([1, 20, 512, 512]), Label: 0\n",
      "Sample 2: Image shape: torch.Size([1, 20, 512, 512]), Label: 0\n",
      "Batch image shape: torch.Size([4, 1, 20, 512, 512])\n",
      "Batch label shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "from pre_processing_py import HandScanDataset2, transform, validation_transform, train_df, valid_df, training_data_dir, display_images\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.65.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (4.25.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/eleanorbolton/.local/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/eleanorbolton/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the swinTransformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from einops import rearrange\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention3D(nn.Module):\n",
    "    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The temporal length, height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wd, Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads)\n",
    "        )  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_d = torch.arange(self.window_size[0])\n",
    "        coords_h = torch.arange(self.window_size[1])\n",
    "        coords_w = torch.arange(self.window_size[2])\n",
    "        coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))  # 3, Wd, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "\n",
    "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        index_clone = self.relative_position_index.clone()\n",
    "        relative_position_bias = self.relative_position_bias_table[index_clone[:N, :N].reshape(-1)].reshape(\n",
    "            N, N, -1\n",
    "        )  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n",
    "\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)  # B_, nH, N, N\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn.float() @ v.float()).transpose(1, 2).reshape(B_, N, C)\n",
    "        # x = self.proj(x.half())\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WindowPartition and WindowReverse Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, D, H, W, C)\n",
    "        window_size (tuple[int]): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (B*num_windows, window_size*window_size, C)\n",
    "    \"\"\"\n",
    "    print(f\"This Input shape: {x.shape}\")\n",
    "    \n",
    "    B, D, H, W, C = x.shape\n",
    "    print(f\"B: {B}\")\n",
    "    print(f\"C: {C}\")\n",
    "    print(f\"D: {D}\")\n",
    "    print(f\"H: {H}\")\n",
    "    print(f\"W: {W}\")\n",
    "\n",
    "    \n",
    "    x = x.view(\n",
    "        B,\n",
    "        D // window_size[0],\n",
    "        window_size[0],\n",
    "        H // window_size[1],\n",
    "        window_size[1],\n",
    "        W // window_size[2],\n",
    "        window_size[2],\n",
    "        C,\n",
    "    )\n",
    "    \n",
    "    print(f\"Shape after view operation: {x.shape}\")\n",
    "    \n",
    "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, reduce(mul, window_size), C)\n",
    "    print(f\"Shape after permute and final view: {windows.shape}\")\n",
    "    \n",
    "    return windows\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, B, D, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (B*num_windows, window_size[0]*window_size[1]*window_size[2], C)\n",
    "        window_size (tuple[int]): Window size\n",
    "        B (int): Batch size\n",
    "        D (int): Depth\n",
    "        H (int): Height\n",
    "        W (int): Width\n",
    "\n",
    "    Returns:\n",
    "        x: (B, D, H, W, C)\n",
    "    \"\"\"\n",
    "    # Reverse the reshape operation\n",
    "    x = windows.view(B, D // window_size[0], H // window_size[1], W // window_size[2],\n",
    "                     window_size[0], window_size[1], window_size[2], -1)\n",
    "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n",
    "    print(f\"Shape after reversing windows: {x.shape}\")  # Debugging\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_window_size Function\n",
    "This function adjusts the window size according to the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_size(x_size, window_size, shift_size=None):\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        use_shift_size = list(shift_size)\n",
    "    for i in range(len(x_size)):\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_mask Function\n",
    "This function computes the attention mask for the shifted windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_mask(D, H, W, window_size, shift_size, device):\n",
    "    print(f\"Dimensions (D, H, W): ({D}, {H}, {W})\")\n",
    "    print(f\"Window size: {window_size}\")\n",
    "    print(f\"Shift size: {shift_size}\")\n",
    "\n",
    "    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n",
    "    print(f\"Initial img_mask shape: {img_mask.shape}\")\n",
    "\n",
    "    cnt = 0\n",
    "    for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "        for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "            for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                print(f\"Filling img_mask with cnt={cnt} for slices d={d}, h={h}, w={w}\")\n",
    "                img_mask[:, d, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "    print(f\"img_mask after filling: {img_mask.unique()}\")  # Shows the unique values in the mask\n",
    "\n",
    "    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n",
    "    print(f\"mask_windows shape after partitioning: {mask_windows.shape}\")\n",
    "\n",
    "    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n",
    "    print(f\"mask_windows shape after squeezing: {mask_windows.shape}\")\n",
    "\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    print(f\"attn_mask shape after subtraction: {attn_mask.shape}\")\n",
    "\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "    print(f\"Final attn_mask shape: {attn_mask.shape}\")\n",
    "\n",
    "    return attn_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchMerging Class\n",
    "The PatchMerging class is used to downsample the spatial dimensions of the feature maps while increasing the channel dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, D, H, W, C).\n",
    "        \"\"\"\n",
    "        # # print(f\"Original input shape: {x.shape}\")\n",
    "        \n",
    "        B, D, H, W, C = x.shape\n",
    "\n",
    "\n",
    "        # Splitting the input into 8 parts and concatenating them along the channel dimension\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 1::2, 0::2, :]\n",
    "        x5 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        \n",
    "        print(f\"Shapes of split parts: {[x0.shape, x1.shape, x2.shape, x3.shape, x4.shape, x5.shape, x6.shape, x7.shape]}\")\n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)  # B D/2 H/2 W/2 8*C\n",
    "        print(f\"Shape after concatenation: {x.shape}\")\n",
    "\n",
    "        x = self.norm(x)\n",
    "        print(f\"Shape after normalization: {x.shape}\")\n",
    "\n",
    "        x = self.reduction(x)\n",
    "        print(f\"Shape after linear reduction: {x.shape}\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Build the Transformer Layers and Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock3D(nn.Module):\n",
    "    \"\"\"Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (tuple[int]): Window size.\n",
    "        shift_size (tuple[int]): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size=(2, 7, 7),\n",
    "        shift_size=(0, 0, 0),\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        assert 0 <= self.shift_size[0] < self.window_size[0], \"shift_size must in 0-window_size\"\n",
    "        assert 0 <= self.shift_size[1] < self.window_size[1], \"shift_size must in 0-window_size\"\n",
    "        assert 0 <= self.shift_size[2] < self.window_size[2], \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention3D(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward_part1(self, x, mask_matrix):\n",
    "        B, D, H, W, C = x.shape\n",
    "        window_size, shift_size = get_window_size((D, H, W), self.window_size, self.shift_size)\n",
    "        x = self.norm1(x)\n",
    "        pad_l = pad_t = pad_d0 = 0\n",
    "        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n",
    "        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n",
    "        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
    "        _, Dp, Hp, Wp, _ = x.shape\n",
    "        # cyclic shift\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, *(window_size + (C,)))\n",
    "        shifted_x = window_reverse(attn_windows, window_size, B, Dp, Hp, Wp)  # B D' H' W' C\n",
    "        # reverse cyclic shift\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :D, :H, :W, :].contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward_part2(self, x):\n",
    "        return self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, D, H, W, C).\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "\n",
    "        shortcut = x\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix)\n",
    "        else:\n",
    "            x = self.forward_part1(x, mask_matrix)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "\n",
    "        if self.use_checkpoint:\n",
    "            x = x + checkpoint.checkpoint(self.forward_part2, x)\n",
    "        else:\n",
    "            x = x + self.forward_part2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(D, H, W, window_size, shift_size, device):\n",
    "    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n",
    "    cnt = 0\n",
    "    for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "        for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "            for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                img_mask[:, d, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "    # pdb.set_trace()\n",
    "    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n",
    "    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "    return attn_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasicLayer Class\n",
    "The BasicLayer class handles the core operations in the Swin Transformer for each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of feature channels\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (tuple[int]): Local window size. Default: (1,7,7).\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size=(7, 7, 7),\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = tuple(i // 2 for i in window_size)\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock3D(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=(0, 0, 0) if (i % 2 == 0) else self.shift_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.downsample = downsample\n",
    "        if self.downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, C, D, H, W).\n",
    "        \"\"\"\n",
    "        # calculate attention mask for SW-MSA\n",
    "\n",
    "        B, C, D, H, W = x.shape\n",
    "        window_size, shift_size = get_window_size((D, H, W), self.window_size, self.shift_size)\n",
    "        x = rearrange(x, \"b c d h w -> b d h w c\")\n",
    "        Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n",
    "        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n",
    "        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n",
    "        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n",
    "        for blk in self.blocks:\n",
    "            # pdb.set_trace()\n",
    "            x = blk(x, attn_mask)\n",
    "        x = x.view(B, D, H, W, -1)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        x = rearrange(x, \"b d h w c -> b c d h w\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from monai.networks.blocks import UnetBasicBlock\n",
    "\n",
    "class PatchEmbed3D(nn.Module):\n",
    "    \"\"\"Video to Patch Embedding.\n",
    "\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: (2,4,4).\n",
    "        in_chans (int): Number of input video channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=(4, 4, 4), in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = UnetBasicBlock(\n",
    "            spatial_dims=3,\n",
    "            in_channels=in_chans,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            norm_name=(\"INSTANCE\", {\"affine\": True}),\n",
    "        )\n",
    "        # self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self.proj(x)  # B C D Wh Ww\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Layers into the FullSwin Transformer 3D model. Integrate these Swin Transformer blocks into a model that can process 3D medical images from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=(20, 512, 512),\n",
    "        patch_size=(2, 4, 4),\n",
    "        in_chans=1,\n",
    "        num_classes=2,\n",
    "        embed_dim=512,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[8, 16, 32, 64],  # Adjusted to match embed_dim\n",
    "        window_size=(2, 16, 16),    # Adjusted to better fit the image size\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.2,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        patch_norm=False,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.patch_embed = PatchEmbed3D(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None,\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(len(depths)):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if i_layer < len(depths) - 1 else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(int(embed_dim * 2 ** (len(depths) - 1)))\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(int(embed_dim * 2 ** (len(depths) - 1)), num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x.mean(dim=[2, 3, 4]).transpose(1, 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.activation_count = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.activation_count += 1\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Instantiate EarlyStopper\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eleanorbolton/anaconda3/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/Users/eleanorbolton/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Epoch 1/10:   0%|          | 0/172 [00:00<?, ?batch/s]/Users/eleanorbolton/anaconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Input shape: torch.Size([1, 10, 256, 256, 1])\n",
      "B: 1\n",
      "C: 1\n",
      "D: 10\n",
      "H: 256\n",
      "W: 256\n",
      "Shape after view operation: torch.Size([1, 10, 1, 16, 16, 16, 16, 1])\n",
      "Shape after permute and final view: torch.Size([2560, 256, 1])\n",
      "This Input shape: torch.Size([1, 10, 256, 256, 512])\n",
      "B: 1\n",
      "C: 512\n",
      "D: 10\n",
      "H: 256\n",
      "W: 256\n",
      "Shape after view operation: torch.Size([1, 10, 1, 16, 16, 16, 16, 512])\n",
      "Shape after permute and final view: torch.Size([2560, 256, 512])\n",
      "Shape after reversing windows: torch.Size([1, 10, 256, 256, 512])\n",
      "This Input shape: torch.Size([1, 10, 256, 256, 512])\n",
      "B: 1\n",
      "C: 512\n",
      "D: 10\n",
      "H: 256\n",
      "W: 256\n",
      "Shape after view operation: torch.Size([1, 10, 1, 16, 16, 16, 16, 512])\n",
      "Shape after permute and final view: torch.Size([2560, 256, 512])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast  # Import for mixed precision training\n",
    "\n",
    "# Define the output directory and log directory\n",
    "output_dir = \"./output\"\n",
    "log_dir = \"./logs\"\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"swin_transformer_3d\")\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "learning_rate = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize dataset and data loader\n",
    "train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize validation dataset and data loader\n",
    "valid_dataset = HandScanDataset2(labels_df=valid_df, data_dir=training_data_dir, transform=validation_transform)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = SwinTransformer3D(\n",
    "    img_size=(4, 512, 512),   # Updated input size to match the new depth\n",
    "    patch_size=(1, 4, 4),     # Adjusted patch size for the new depth\n",
    "    in_chans=1,               # Number of input channels\n",
    "    num_classes=2,            # Number of classes for classification\n",
    "    embed_dim=512,            # Embedding dimension\n",
    "    depths=[2, 2, 4, 2],      # Adjusted depths per stage\n",
    "    num_heads=[8, 16, 32, 64],# Number of attention heads per stage\n",
    "    window_size=(1, 16, 16),  # Adjusted window size to match the new depth\n",
    "    mlp_ratio=4.0,            # MLP ratio\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    drop_path_rate=0.2,\n",
    "    norm_layer=nn.LayerNorm,\n",
    "    patch_norm=False,\n",
    "    use_checkpoint=False\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Initialize GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, images)\n",
    "\n",
    "            # Backward pass and optimization with mixed precision\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Logging every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    # Log average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    logger.info(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch_idx, (images, labels) in enumerate(valid_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    # Log average validation loss for the epoch\n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "    logger.info(f'Epoch [{epoch+1}/{num_epochs}], Average Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Time taken for the epoch\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    logger.info(f'Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds')\n",
    "\n",
    "     # Early stopping check\n",
    "    if early_stopper.early_stop(avg_val_loss):\n",
    "        # Save model parameters if early stopping criteria are met for the first time\n",
    "        if early_stopper.activation_count == 1:\n",
    "            logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\n",
    "            # Save network parameters and losses\n",
    "            opt_model_filepath = f\"{output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\"\n",
    "            Early_Stop_Epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), opt_model_filepath)\n",
    "        \n",
    "\n",
    "    # Switch back to training mode\n",
    "    model.train()\n",
    "\n",
    "# Closing the writer at the end\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/43 [00:00<?, ?batch/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Define the output directory and log directory\n",
    "output_dir = \"./output\"\n",
    "log_dir = \"./logs\"\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"swin_transformer_3d\")\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "learning_rate = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize dataset and data loader\n",
    "train_dataset = HandScanDataset2(labels_df=train_df, data_dir=training_data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = SwinTransformer3D(\n",
    "    img_size=(20, 512, 512),  # Adjust according to your input size\n",
    "    patch_size=(2, 4, 4),     # Adjusted patch size\n",
    "    in_chans=1,               # Number of input channels\n",
    "    num_classes=2,            # Number of classes for classification\n",
    "    embed_dim=512,            # Embedding dimension\n",
    "    depths=[2, 2, 6, 2],      # Depths per stage\n",
    "    num_heads=[8, 16, 32, 64],# Number of attention heads per stage\n",
    "    window_size=(2, 16, 16),  # Window size matching model config\n",
    "    mlp_ratio=4.0,            # MLP ratio\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    drop_path_rate=0.2,\n",
    "    norm_layer=nn.LayerNorm,\n",
    "    patch_norm=False,\n",
    "    use_checkpoint=False\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)  # Remove attn_mask from here, as it's handled internally\n",
    "            loss = criterion(outputs, images)  # Assuming you have labels, use them instead of images\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Logging every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    # Log average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    logger.info(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Time taken for the epoch\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    logger.info(f'Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds')\n",
    "\n",
    "# Closing the writer at the end\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Compute mask matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m attn_mask \u001b[39m=\u001b[39m compute_mask(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     D\u001b[39m=\u001b[39mimages\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     H\u001b[39m=\u001b[39mimages\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     W\u001b[39m=\u001b[39mimages\u001b[39m.\u001b[39mshape[\u001b[39m4\u001b[39m], \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     window_size\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mwindow_size, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     shift_size\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mshift_size, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     device\u001b[39m=\u001b[39mimages\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eleanorbolton/Documents/project_repo/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/Predicting-Rheumatoid-Arthritis-Progression-Using-Hand-MRI-Scans-from-At-Risk-Patients/predicting-rheumatoid-arthritis-progression-using-hand-mri-scans-from-at-risk-patients/src/models/DAE.ipynb#X50sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(inputs, outputs, n=4):\n",
    "    \"\"\"\n",
    "    Function to visualize input and output images side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs (Tensor): The input images tensor of shape (B, C, D, H, W).\n",
    "    - outputs (Tensor): The output images tensor of shape (B, C, D, H, W).\n",
    "    - n (int): Number of images to display.\n",
    "    \"\"\"\n",
    "    inputs = inputs.cpu().detach()\n",
    "    outputs = outputs.cpu().detach()\n",
    "\n",
    "    fig, axes = plt.subplots(n, 2, figsize=(10, n*5))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Display the input image (choose the middle slice for visualization)\n",
    "        axes[i, 0].imshow(inputs[i, 0, inputs.shape[2]//2], cmap='gray')\n",
    "        axes[i, 0].set_title(f'Input Image {i+1}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Display the output image (choose the middle slice for visualization)\n",
    "        axes[i, 1].imshow(outputs[i, 0, outputs.shape[2]//2], cmap='gray')\n",
    "        axes[i, 1].set_title(f'Output Image {i+1}')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of data\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "# Compute mask matrix\n",
    "attn_mask = compute_mask(\n",
    "    D=images.shape[2], \n",
    "    H=images.shape[3], \n",
    "    W=images.shape[4], \n",
    "    window_size=model.window_size, \n",
    "    shift_size=model.shift_size, \n",
    "    device=images.device\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(images, attn_mask)\n",
    "\n",
    "# Visualize the input and output images\n",
    "show_images(images, outputs, n=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
